\documentclass{report}

\usepackage{makecell}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathrsfs}
%\usepackage[hangul]{kotex}
%\usepackage{kotex-logo}
\usepackage{float}
\usepackage[left=2.5cm,right=2.5cm,top=3cm,bottom=3cm,a4paper]{geometry}
\usepackage{fancyhdr}
\usepackage{indentfirst}
\usepackage{bookmark}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    pdftitle={Sharelatex Example},
    bookmarks=true
}
\bookmarksetup{
  numbered, 
  open,
}


\newtheorem{ex}{}[section]
\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}


\makeatletter

\newcommand\frontmatter{%
    \clearpage
  \pagenumbering{roman}}

\newcommand\mainmatter{%
    \clearpage
  \pagenumbering{arabic}}

\newcommand\backmatter{%
  \if@openright
    \cleardoublepage
  \else
    \clearpage
  \fi
   }
   
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\footrulewidth}{0.5pt}
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\thepage}
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.5pt}
}




\begin{document}
\title{Probability Theory and Example Solution Manual}
\author{}
\date{\today}
\frontmatter
\maketitle


\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\mainmatter

\chapter{Measure Theory}
\section{Probability Spaces}
\begin{ex}
Let $\Omega = \mathbb{R}, \mathcal{F} =$ all subsets so that $A$ or $A^c$ is countable, $P(A) = 0$ in the first case and = 1 in the second. Show that $(\Omega, \mathcal{F}, P)$ is a probability space.
\end{ex}
\begin{proof}[sol]
i) $\mathcal{F}$ is a $\sigma-$algebra on $\mathbb{R}$.

$\emptyset \in \mathcal{F}$ since $\emptyset$ is countable.

By definition, $\mathcal{F}$ is closed under complementations.

Countable union of countable sets is countable.  Union of countable sets and uncountable sets is uncountable. Thus, $\mathcal{F}$ is closed under countable union. 

ii) $P$ is a probability measure.

$P(\emptyset) = 0$ since $\emptyset$ is countable. By definition, for any set $A$, $P(A) \ge 0$.

Countable union of countable sets is countable.  Union of countable sets and uncountable sets is uncountable. Thus, $P$ has the countable additivity property. 

If $A$ is countable, then $A^c$ is uncountable since $\Omega$ is uncountable. Thus,
\[P(\Omega) = P(A) + P(A^c) = 1\]
By (i) and (ii), $(\Omega, \mathcal{F}, P)$ is a probability space.
\end{proof}
\begin{ex}
Recall the definition of $\mathcal{S}_d$ from Example 1.1.5. Show that $\sigma (\mathcal{S}_d) =\mathcal{R}^d$, the Borel subsets of $\mathbb{R}^d$
\end{ex}
\begin{proof}[sol]~
i) $d = 1$. In $\mathbb{R}$, any open set can be represented by countable union of open intervals. Thus, we need to show that any open interval can be represented by elements of $\mathcal{S}$. Let $-\infty < a <  b < \infty$.
\[(a,b) = (-\infty, a]^c \cup (\cup_i(-\infty, b - 1/n])\]
If $a = -\infty$, then $(a,b)$ can be represented by the second term. If $b =  \infty$, then $(a,b)$ can be represented by the first term. Thus, $\sigma(\mathcal{S}) = \sigma(\mathcal{R}) = \mathcal{R}$.

ii) $d \ge 2$. $\mathcal{S}_d$ is a finite cartesian product of $\mathcal{S}$. Similarly, $\sigma(\mathcal{S}_d) = \sigma(\mathcal{R}_d) = \mathcal{R}_d$.
\end{proof}
\begin{ex}
A $\sigma$-field F is said to be countably generated if there is a countable collection $C \subset F$ so that $\sigma(C) = F$. Show that $\mathcal{R}^d$ is countably generated.
\end{ex}
\begin{proof}[sol]
Let $C$ be the collection that contains all sets of the form
\[[q_1,\infty)\times \dotsb\times[q_d,\infty), (q_1,\dotsc,q_d) \in \mathbb{Q}^d\]
Then, $C$ is countable, since it is finite union of countable sets. And $\sigma(C) = \mathcal{R}^d$ as presented by previous exercise.
\end{proof}
\begin{ex}
(i) Show that if $F_1 \subset F_2 \subset \dotsc $ are $\sigma$-algebras, then $\cup_i{F}_i$ is an algebra. (ii) Give an example to show that $\cup_iF_i$ need not be a $\sigma$-algebra.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] By definition, $\cup_i F_i$ is not empty.
    Choose $x \in \cup_i F_i$. Then, there exists $F_x$ such that $x \in F_x$. Therefore, $x^c \in F_x \subset \cup_i F_i$. Thus, $\cup_i F_i$ is closed under complementations.
    
    Choose, $x \in F_i, y \in F_j$ and suppose $i\le j$. Then $x \in F_j$. Thus, $x \cup y \in F_j \Rightarrow x \cup y \in \cup_i F_i$.  Thus, $\cup_i F_i$ is closed under union.
    \item[(ii)] Let $F_i = \sigma(\{\{1\},\{2\},\dotsb,\{n\}\})$. Let $A = \{\{n\} : n = 3k| k = 1,2,3,\dotsb\}$. Then for all $i$, $A \notin F_i$. Thus, $A \not\in \cup_i F_i$. However, $A$ can be represented by countable union. Therefore, $\cup_iF_i$ is not a $\sigma-$algebra.
\end{enumerate}
\end{proof}
\begin{ex}
A set $A \subset \{1, 2, \dotsc \}$ is said to have asymptotic density $\theta$ if
\[\lim_{n\to\infty} |A \cap \{1, 2,\dotsb , n\}|/n = \theta\]
Let $\mathcal{A}$ be the collection of sets for which the asymptotic density exists.
Is $\mathcal{A}$ a $\sigma$-algebra? an algebra?
\end{ex}
\begin{proof}[sol]
Let $A$ be the set of even numbers. Next, we construct a set $B$ in the following way: we begin with $\{2, 3\}$ and starting with $k = 2$, take all even numbers $2^k < n \le (3/2) \times 2^k$
, and all odd numbers$(3/2) \times 2^k < n \le 2^{k+1}$. Then, the asymptotic density of $B$ is 0.5. However, the asymptotic density $A\cap B$ does not exists.

When $n = (3/2)\times 2^k$, then the density is 1/3. When $n = 2^{k+1}$, then the density is 1/4.

Thus, $\mathcal{A}$ is not closed under intersection. $\mathcal{A}$ is neither $\sigma-$algebra nor algebra
\end{proof}
\section{Distributions}
\begin{ex}
Suppose $X$ and $Y$ are random variables on $(\Omega,\mathcal{F}, P)$ and let $A \in \mathcal{F}$. Show that if we let $Z(\omega) = X(\omega)$ for $\omega \in A$ and $Z(\omega) = Y (\omega)$ for $\omega \in A^c$, then $Z$ is a random variable.
\end{ex}
\begin{proof}[sol]
Let the Borel set $B$ which satisfies that
\[\text{if } \omega \in A, \text{ then } Z(\omega) \in B\]
For arbitrary Borel set $S$, $S = (S\cap B)\cup (S\cap B^c)$. Since $S$ is Borel set, $\{\omega : X(\omega) \in S\}, \{\omega : Y(\omega) \in S\} \in \mathcal{F}$.
\begin{align*}
    \{\omega : Z(\omega) \in S\} &= \{\omega : Z(\omega) \in (S\cap B)\} \cup \{\omega : Z(\omega) \in (S\cap B^c)\}\\
    &= \{\omega : X(\omega) \in (S\cap B)\} \cup \{\omega : Y(\omega) \in (S\cap B^c)\}
\end{align*}
Since $\mathcal{F}$ is closed on set operation, $\{\omega : Z(\omega) \in S\} \in \mathcal{F}$. 
\end{proof}
\begin{ex}
Let $\chi$ have the standard normal distribution. Use Theorem 1.2.6 to get upper and lower bounds on P($\chi \ge $ 4).
\end{ex}
\begin{proof}[sol]
\begin{align*}
    P(\chi \ge 4) = (2\pi)^{-1}\int_4^\infty \exp(-y^2/2)dy &\le (8\pi)^{-1}\exp(-8)\\
    (2\pi)^{-1}\int_4^\infty \exp(-y^2/2)dy &\ge (15/128\pi)\exp(-8)
\end{align*}
\end{proof}
\begin{ex}
Show that a distribution function has at most countably many discontinuities.
\end{ex}
\begin{proof}[sol]
Let $D$ be the set of discontinuity points. Choose $x, y \in D$. Then we can choose rational number $q_x \in (F(x-), F(x+))$. Since $F$ is increasing, if $x\ne y$, then $q_x \ne q_y$. Thus $x \to q_x$ is one-to-one function. Since $\mathbb{Q}$ is countable, $D$ is at most countable.
\end{proof}
\begin{ex}
Show that if $F(x) = P(X \le x)$ is continuous then $Y = F(X)$ has a uniform distribution on (0,1), that is, if $y \in [0, 1]$, $P(Y \le y) = y$.
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \{\omega | Y(\omega) \le y\} &= \{\omega | F(X(\omega)) \le y\}\\
    &= \{\omega | X(\omega) \le k\} \quad k = \inf\{x | F(x) \ge y \}\\
    P(\{\omega | Y(\omega) \le y\}) &= P(\{\omega | X(\omega) \le k\})\\
    &= P(X \le k) = y
\end{align*}
\end{proof}
\begin{ex}
Suppose $X$ has continuous density $f$, $P(\alpha \le X \le \beta) = 1$ and $g$ is a function that is strictly increasing and differentiable on $(\alpha, \beta)$. Then $g(X)$ has density $f(g^{-1}(y))/g'(g^{-1}(y))$ for $y \in (g(\alpha), g(\beta))$ and 0 otherwise. When $g(x) = ax + b$ with $a > 0, g^{-1}(y) = (y - b)/a$ so the answer is $(1/a)f((y - b)/a)$.
\end{ex}
\begin{proof}[sol]
Since $g$ is strictly increasing $g^{-1}$ exists.
\begin{align*}
    P(g(X) \le y) &= P(X \le g^{-1}(y)) = \int_\alpha^{g^{-1}(y)} f(x)dx\\
    \frac{d}{dy}P(g(X) \le y) &= f(g^{-1}(y))\frac{d}{dy}g^{-1}(y) = f(g^{-1}(y))/g'(g^{-1}(y))
\end{align*}
\end{proof}
\begin{ex}
Suppose $X$ has a normal distribution. Use the previous exercise to compute the density of $\exp(X)$.
\end{ex}
\begin{proof}[sol]
Let $g(x) = \exp(x)$. Then $g^{-1}(x) = \log(x)$
\begin{align*}
    f(g^{-1}(x))/g'(g^{-1}(x)) &= f(\log(x))/\exp(\log(x))\\
    &= f(\log(x))/x = \frac{1}{x\sqrt{2\pi}}\exp(-\log(x)^2/2), \ x > 0
\end{align*}
\end{proof}
\section{Random Variables}
\begin{ex}
Show that if $\mathcal{A}$ generates $\mathcal{S}$, then $X^{-1}(\mathcal{A}) \equiv \{\{X \in A\} : A \in \mathcal{A}\}$ generates $\sigma(X) = \{\{X \in B\} : B \in \mathcal{S}\}$.
\end{ex}
\begin{proof}[sol]
By assumption, $\mathcal{A} \subset \mathcal{S}$. Thus,  $X^{-1}(\mathcal{A})\subset X^{-1}(\mathcal{S})$. Therefore, $\sigma(X^{-1}(\mathcal{A}))\subset\sigma( X^{-1}(\mathcal{S})) = \sigma(X)$.

Choose a set $B \in \mathcal{S}$. Since $\mathcal{A}$ generates $\mathcal{S}$, $B$ can be represented by sets of $\mathcal{A}$. Thus, $\{X \in B\} \in X^{-1}(\mathcal{A})$. Therefore, $\sigma(X) \in X^{-1}(\mathcal{A})$.
\end{proof}
\begin{ex}
Prove Theorem 1.3.6 when $n = 2$ by checking $\{X_1+X_2 < x\} \in \mathcal{F}$.
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \{X_1+X_2 < x\} = \bigcup_{q \in \mathbb{Q}} \{X_1 < q\}\times \{X_2 < x-q\}
\end{align*}
Both $\{X_1 < q\}$ and $\{X_2 < x-q\}$ are open sets. Thus, $\{X_1+X_2 < x\} \in \mathcal{R}^2$ since it is represented by countable union.
\end{proof}
\begin{ex}
Show that if $f$ is continuous and $X_n \to X$ almost surely then $f(X_n) \to f(X)$ almost surely.
\end{ex}
\begin{proof}[sol]
Let $\Omega_0 = \{\omega : \lim X_n(\omega) = X(\omega)\}$ and $\Omega_f = \{\omega : \lim f(X_n(\omega)) = f(X(\omega))\}$.
\begin{align*}
    \omega \in \Omega_0 &\Rightarrow \lim X_n(\omega) = X(\omega)\\
    &\Rightarrow \lim f(X_n(\omega)) = f(X(\omega))  \ \ \because f\text{ is continuous}\\
    &\Rightarrow \omega \in \Omega_f\\
    &\Rightarrow 1 = P(\Omega_0) \le P(\Omega_f)\le 1
\end{align*}
\end{proof}
\begin{ex}
(i) Show that a continuous function from $\mathcal{R}^d \to \mathcal{R}$ is a measurable map from ($\mathbb{R}^d,\mathcal{R}^d)$ to $(\mathbb{R},\mathcal{R})$. (ii) Show that $\mathcal{R}^d$ is the smallest $\sigma$-field that makes all the continuous functions measurable.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] $\mathcal{R}$ is $\sigma$-field which is generated by all open sets. If $f$ is continuous mapping and image is open, then inverse-image is also open. Thus, $f$ is a measurable map.
    \item[(ii)] Let $\mathcal{S}$ be the smallest $\sigma-$field that makes all the continuous functions measurable. Since $\mathcal{R}$ is the smallest $\sigma-$field which is generated by all open sets,  $\mathcal{S}$ is generated by all open sets in $\mathbb{R}^d$.
\end{enumerate}
\end{proof}
\begin{ex}
A function $f$ is said to be lower semicontinuous or l.s.c. if
\[\liminf_{y\to x} f(y) \ge f(x)\]
and upper semicontinuous (u.s.c.) if $-f$ is l.s.c.. Show that f is l.s.c. if
and only if $\{x : f(x) \le a\}$ is closed for each $a \in \mathbb{R}$ and conclude that
semicontinuous functions are measurable.
\end{ex}
\begin{proof}[sol]~
Define $E_a := \{x : f(x) \le a\}$
\begin{enumerate}
    \item[if] Choose $x$. Let $a = \liminf_{y\to x}{f(y)}$. Then, by definition, $a \le f(x)$.
    There is the sequence $\{x_n\}$ such that $x_n \to x, f(x_n) \to a$ as $n \to \infty$
    For any $b > a$, there exists $N_b$ such that for all $n > N_b$, $x_n \in E_b$. Since $E_b$ is closed, the limit point $x \in E_b$ for all $b > a$. Thus, $x \in E_a$. Since $f(x) \ge a$ and $f(x) \le a$, $f(x) = a = \liminf_{y\to x} f(y)$. 
    \item[only if] Construct a sequence $\{x_n\}$ in $E_a$ which converges to $x$. Since $f$ is l.s.c
    \[f(x) \le \liminf_{y\to x} f(y) \le a\]
    Thus, $x \in E_a$. Since an arbitrary limit point is in $E_a$, $E_a$ is closed.
\end{enumerate}
\end{proof}
\begin{ex}
Let $f : \mathbb{R}^d \to \mathbb{R}$ be an arbitrary function and let $f^\delta (x) = \sup \{f(y) : |y - x| < \delta\}$ and $f_\delta(x) = \inf \{f(y) : |y - x| < \delta\}$ where $|z| = (z_1^2 + ... + z_d^2)^{1/2}$. Show that $f^\delta$ is l.s.c. and $f_\delta$ is u.s.c. Let $f^0 = \lim_{\delta \downarrow0}f^\delta$, $f_0 = \lim_{\delta \downarrow0}f_\delta$ and conclude that the set of points at which $f$ is discontinuous = $\{f^0 \ne f_0\}$ is measurable.
\end{ex}
\begin{proof}[sol]
By using result of previous exercise, we need to show that $\{x : f^\delta (x) \le a\}$ and $\{x : f_\delta(x) \ge a\}$ are closed for each $a \in \mathbb{R}$.

We can construct a sequence $\{x_n\} \subset \{t : f^\delta(t) \le (a)\}$ which converges to $x$. Choose $y \in \{t : |t - x| < \delta\}$. Then, we can find some $N$ which satisfies for all $n \ge N$, $|x_n - x| < \delta - |y - x|$. Thus, $|x_n - y| \le |x_n - x| + |y - x| <\delta \Rightarrow f(y) \le a$. Since $y$ is arbitrary chosen, $\sup\{f(y): |y-x| < \delta\} \le a$. The limit of convergent sequence is in $\{t : f^\delta(t) \le (a)\}$. Thus, $\{t : f^\delta(t) \le (a)\}$ is closed. We can prove that $\{x : f_\delta(x) \ge a\}$ is closed similarly.

Since $\{f^0 \ne f_0\} = \{x : f^0(x) = f_0(x)\}^c$, we need to show that $\{x : f^0(x) = f_0(x)\}$ is measurable. We showed that $\mathcal{S}$ can be generated by half closed intervals(ex 1.1.2). Thus, $f^0$ and $f_0$ are measurable functions.
\end{proof}
\begin{ex}
A function $\varphi : \Omega \to \mathbb{R}$ is said to be simple if
\[\varphi(\omega) = \sum_{m=1}^n c_m1_{A_m}(\omega)\]
where the $c_m$ are real numbers and $A_m \in \mathcal{F}$. Show that the class of $\mathcal{F}$ measurable functions is the smallest class containing the simple functions and closed under pointwise limits.
\end{ex}
\begin{proof}[sol]
By definition, simple functions are $\mathcal{F}$ measurable.
\end{proof}
\begin{ex}
Use the previous exercise to conclude that $Y$ is measurable with respect to $\sigma(X)$ if and only if $Y = f(X)$ where $f : \mathbb{R} \to \mathbb{R}$ is measurable
\end{ex}
\begin{ex}
To get a constructive proof of the last result, note that $\{\omega : m2^{-n} \le Y < (m+1)2^{-n}\} = \{X \in B_{m,n}\}$ for some $B_{m,n} \in \mathcal{R}$ and set $f_n(x) = m2^{-n}$ for $x \in B_{m,n}$ and show that as $n \to \infty$, $f_n(x) \to f(x)$ and $Y = f(X)$.
\end{ex}
\section{Integration}
\begin{ex}
Show that if $f \ge 0$ and $\int f d\mu = 0$ then $f = 0$ a.e.
\end{ex}
\begin{proof}[sol]
Let $N$ be the set that satisfies $x \in N \Rightarrow f(x) = 0$.
\begin{align*}
    \int f d\mu &=  \int_N f d\mu  + \int_{N^c} f d\mu\\
    &= \int_{N^c} f d\mu = 0\\
    &\Rightarrow \mu(N^c) = 0
\end{align*}
\end{proof}
\begin{ex}
Let $f \ge 0$ and $E_{n,m} = \{x : m/2^n \le f(x) < (m+1)/2^n\}$. As $n \uparrow \infty$,
\[\sum_{m=1}^\infty \frac{m}{2^n} \mu(E_{n,m}) \uparrow \int f d\mu\]
\end{ex}
\begin{proof}[sol]
Let $f_n = \sum_{m=1}^\infty 1_{E_{n,m}}$. Then, for any n, $f_n < f$. Thus, $\int f d\mu$ is an upper bound of $\int f_n d\mu$. Since $\int f_n d\mu$ is increasing and bounded, it converges. By definition,
\begin{align*}
    \int f -f_n d\mu \le \int 1/2^n d\mu \to 0 \text{ as } n \to \infty
\end{align*}
Thus, $\int f_n d\mu$ converges to $\int f d\mu$
\end{proof}
\begin{ex}
Let $g$ be an integrable function on $\mathbb{R}$ and $\epsilon > 0$. (i) Use the definition of the integral to conclude there is a simple function $\varphi = \sum_k b_k1_{A_k}$ with $\int |g - \varphi| dx < \epsilon$. (ii) Use Exercise A.2.1 to approximate the $A_k$ by finite unions of intervals to get a step function
\[q = \sum_{j=1}^k c_j 1_{(a_{j-1}, a_j)}\]
with $a_0 < a_1 < \dotsb < a_k$, so that $\int |\varphi-q| < \epsilon$. (iii) Round the corners of $q$ to get a continuous function $r$ so that $\int |q - r| dx < \epsilon$.
(iv) To make a continuous function replace each $c_j1_{(a_{j-1},a_j)}$ by a function
that is 0 $(a_{j-1}, a_j)^c$, $c_j$ on $[a_{j-1} + \delta_j, a_j - \delta_j ]$, and linear otherwise. If the $\delta_j$ are small enough and we let $r(x) = \sum^k_{j=1} r_j(x)$ then
\[\int |q(x) - r(x)|d\mu = \sum_{j=1}^k \delta_j c_j <\epsilon\]
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] For partition $P$, we set $f_i(x) = \inf\{f(x) :x \in [x_i, x_{i+1}]\}$. Then, there exists a partition for every $\epsilon$ that makes $\int|\varphi - g|dx < \epsilon$.($\varphi = \sum f_i$)
\end{enumerate}
\end{proof}
\begin{ex}
Prove the Rimann-Lebesgue lemma. If $g$ is integrable then
\[\lim_{n\to\infty} \int g(x) \cos nx dx  = 0\]
\end{ex}
\begin{proof}[sol]
Let $\epsilon$ be an arbitrary positive number. We can find a step function $\varphi$ such that $\int |g - \varphi|dx < \epsilon / 2$.
\begin{align*}
    \int \varphi(x)\cos(nx)dx &= \sum_{i=1}^m c_i\int_{a_i}^{b_i}\cos(nx)dx\\
    &= \sum_{i=1}^m c_i/n\int_{na_i}^{nb_i}\cos(y)dy\\
    &= \sum_{i=1}^m c_i/n(\sin(nb_i) -\sin(na_i)) \to 0
\end{align*}
Thus, we can choose $N$ which satisfies that for all $m \ge N$, $\int|\varphi(x)\cos(nx)|dx < \epsilon/2$. Then,
\begin{align*}
    |\int g(x)\cos(nx)dx| &\le \int|g(x)\cos(nx)|dx\\
    &\le \int|g(x)-\varphi(x)||\cos(nx)|dx + \int|\varphi(x)\cos(nx)|dx\\
    &\le \epsilon/2 + \epsilon/2 = \epsilon
\end{align*}
Since $\epsilon$ is arbitrary, it converges to 0.
\end{proof}
\section{Properties of the integral}
\begin{ex}
Let $\|f\|_\infty = \inf\{M : \mu(\{x : |f(x)| > M\}) = 0\}$. Prove that
\[\int |fg| d\mu \le \|f\|_1\|g\|_\infty\]
\end{ex}
\begin{proof}[sol]
By definition, $g \le \|g\|_\infty$ a.e.. Thus,
\[\int |fg| d\mu \le \int |f|\|g\|_\infty d\mu = \|g\|_\infty \int|f|d\mu = \|f\|_1\|g\|_\infty\]
\end{proof}
\begin{ex}
Show that if $\mu$ is a probability measure then
\[\|f\|_\infty = \lim_{p\to\infty} \|f\|_p\]
\end{ex}
\begin{proof}[sol]
Since $\mu$ is a probability measure, $\mu(\Omega) = 1$.
Suppose two positive numbers, $n < m$. Let $\varphi(x) := x^{m/n}$ By Jensen's inequality,
\[\|f\|_n^m = \varphi(\int |f|^nd\mu) \le \int \varphi(|f|^n)d\mu  = \|f\|_m^m\]
Thus, $\{\|f\|_n\}$ is an increasing sequence.
\[\|f\|_n^n = \int |f|^nd\mu \le \int (\|f\|_\infty^n)d\mu  = \|f\|_\infty^n\]
Thus, $\|f\|_\infty$ is an upper bound. Therefore, the sequence converges and $\lim_{p\to\infty} \|f\|_p \le\|f\|_\infty$.

Let $\epsilon$ be the arbitrary number between 0 and $\|f\|_\infty$. Then, there exist a set $M$ such that $|f| > \|f\|_\infty - \epsilon$ for all $x \in M$.
\begin{align*}
    \int |f|^p d\mu &\ge \int_M |f|^p d\mu \ge (\|f\|_\infty - \epsilon)^p\mu(M)\\
    \Rightarrow \|f\|_p &\ge (\|f\|_\infty - \epsilon)\mu(M)^{1/p}\\
    \Rightarrow \lim_{p\to\infty} \|f\|_p &\ge \|f\|_\infty - \epsilon\\
    \Rightarrow \lim_{p\to\infty} \|f\|_p &\ge \|f\|
\end{align*}
Thus, the sequences converges to $\|f\|_\infty$.
\end{proof}
\begin{ex}[Minkowski's inequality]
(i) Suppose $p \in (1,\infty)$. The inequality $|f+g|^p \le 2^p(|f|^p + |g|^p)$ shows that if $\|f\|^p$ and $\|g\|^p$ are $<\infty$ then $\|f+g\|_p < \infty$. Apply Holder's inequality to $|f||f+g|^{p-1}$ and $|g||f+g|^{p-1}$ to show $\|f+g\|_p \le \|f\|_p + \|g\|_p$ (ii) Show that the last result remains true when $p = 1$ or $p = \infty$
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] By Holder's inequality,
    \begin{align*}
    \int |f||f+g|^{p-1}d\mu &\le \|f\|_p\|(f+g)^{p-1}\|_{p/(p-1)}\\
    &= (\int |f|^pd\mu)^{1/p}(\int |f+g|^pd\mu)^{(p-1)/p}\\
    \text{Similarly, }\int |g||f+g|^{p-1}d\mu &\le (\int |g|^pd\mu)^{1/p}(\int |f+g|^pd\mu)^{(p-1)/p}\\
    \int |f+g|^pd\mu &= \int (|f|+|g|)|f+g|^{p-1}d\mu \le (\|f\|_p+\|g\|_p)\|f+g\|_p^{p-1}\\
    \Rightarrow \|f+g\|_p &\le(\|f\|_p+\|g\|_p)
    \end{align*}
    \item[(ii)] When $p = 1$, it is trivial.
    
    Let the set $E = \{M: \mu(\{x : |f(x)+g(x)| > M\}) = 0\}$. By triangle inequality,
    \[|f+g| \le |f| + |g| \le \|f\|_\infty +\|g\|_\infty \ a.e.\]
    Thus,
    \begin{align*}
        \mu(\{x : |f(x)+g(x)| > \|f\|_\infty + \|g\|_\infty\}) = 0
    \end{align*}
    Since $\|f\|_\infty +\|g\|_\infty\in E$, $\|f\|_\infty +\|g\|_\infty \ge \inf E =\|f+g\|_\infty$
\end{enumerate}
\end{proof}
\begin{ex}
If $f$ is integrable and $E_m$ are disjoint sets with union $E$ then
\[\sum_{m=0}^\infty \int_{E_m} f d\mu = \int_E fd\mu\]
So if $f \ge 0$, then $\nu(E) = \int_E fd\mu$ defines a measure.
\end{ex}
\begin{proof}[sol]~
If $E = \emptyset$, $\mu(E) = 0$. Since $f \ge 0$, for any set $E$, $\nu(E) \ge \nu(\emptyset) = 0$.

If $E_i$ are disjoint sets, then
\[\mu(\cup_i E_i) = \int_{\cup_i E_i} fd\mu = \sum_i\int_{E_i}fd\mu = \sum_i \nu(E_i)\]

Thus, $\nu$ is a measure.
\end{proof}
\begin{ex}
If $g_n \uparrow g$ and $\int g_1^- d\mu < \infty$ then $\int g_n d\mu \uparrow \int gd\mu$.
\end{ex}
\begin{proof}[sol]
Since $g_n = g_n^+ - g_n^-$ and $g_n \uparrow g$, $g_n^+ \uparrow g^+$ and $g_n^- \downarrow g^-$. By monotone convergence theorem, $\int g_n^+d\mu \uparrow \int g^+d\mu$. Since $g_1^-$ is integrable, by dominated convergence theorem, $\int g_n^-d\mu\to \int g^-d\mu$. Thus, $-\int g_n^-d\mu\uparrow -\int g^-d\mu$.
\[\therefore \int g_nd\mu = \int g_n^+ - g_n^- d\mu \uparrow \int gd\mu\]
\end{proof}
\begin{ex}
If $g_m \ge 0$ then $\int \sum_{m=0}^\infty g_m d\mu = \sum_{m=0}^\infty \int g_m d\mu$.
\end{ex}
\begin{proof}[sol]
Define $f_n := \sum_{m=0}^n g_m$ and $f := \sum_{m=0}^\infty g_m$. Then $f_n \uparrow f$. By result of using previous example, $\int f_nd\mu \uparrow \int fd\mu$. Thus,
\begin{align*}
    \int \sum_{m=0}^\infty g_m d\mu &=\int \lim_{n\to\infty}f_nd\mu\\
    &= \int fd\mu\\
    &= \int \sum_{m=0}^\infty g_md\mu
\end{align*}
\end{proof}
\begin{ex}
Let $f \ge 0$. (i) Show that $\int f \land n \ d\mu \uparrow \int fd\mu$ as $n\to\infty$. (ii) Use (i) to conclude that if $g$ is integrable and $\epsilon > 0$ then we can pick $\delta > 0$ so that $\mu(A) < \delta$ implies $\int_A |g|d\mu < \epsilon$.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] 
Define $E_n := \{x : f(x) > n\}$. Then, for $n \le m$
\begin{align*}
   &f \land m -  f \land n = (f-n) 1_{E_n \cap E_m^c} + (m-n)1_{E_m} \ge 0\\
   &\lim_{n\to\infty} f \land n = f
\end{align*}
By definition $f_n \ge 0$. Thus, by monotone convergence theorem, $\int f \land n \ d\mu \uparrow \int fd\mu$.
\item[(ii)] By results of (i), we can find the positive $N$ for any positive $\epsilon/ 2$, $\mu(\{x : |g(x)| > N\}) < \epsilon$. 
\begin{align*}
    \int_A |g|d\mu &\le \int_{\{x : |g(x)| > N\}} |g|d\mu +  \int_{\{x \in A : |g(x)| \le N\}} |g|d\mu\\
    &\le \epsilon /2 + N \mu(A)
\end{align*}
Thus, if we set $\delta := \epsilon /(2N)$, then $\mu(A) < \delta$ implies $\int_A |g|d\mu < \epsilon$.
\end{enumerate}
\end{proof}
\begin{ex}
Show that if $f$ is integrable on $[a,b]$, $g(x) = \int_{[a,x]}f(y)dy$ is continuous on $(a,b)$.
\end{ex}
\begin{proof}[sol]
Since $f$ is integrable, there exists a positive number $M$ such that $|f| < M$. For $s,t \in (a,b), s\le t$
\begin{align*}
-M(t-s) < g(t) - g(s) &= \int^t_s f(y)dy < M(t-s)\\
\Rightarrow |g(t) - g(s)| &< M(t-s)
\end{align*}
Thus, $g$ is Lipschitz continuous. Thus, $g$ is continuous.
\end{proof}
\begin{ex}
Show that if $f$ has $\|f\|_p = (\int |f|^pd\mu)^{1/p} < \infty$, then there are simple functions $\varphi_n$ so that $\|\varphi_n - f\|_p \to 0$.
\end{ex}
\begin{proof}[sol]
For $f$, we can construct sequence of step functions $\{\varphi_n\}$ such that $\varphi_n \uparrow |f|$. By triangle inequailty, $|f - \varphi_n| \le |f| + |\varphi_n|$. Thus, for all $n$
\begin{align*}
    |f - \varphi_n|^p \le (|f| + |\varphi_n|)^p \le (2|f|)^p
\end{align*}
By dominated convergence theorem,
\begin{align*}
    \lim_{n\to\infty}\int |f - \varphi_n|^p &= \int \lim_{n\to\infty}|f - \varphi_n|^p = 0\\
    \Rightarrow \|f - \varphi_n\|_p &= 0
\end{align*}
\end{proof}
\begin{ex}
Show that if $\sum_n \int |f_n|d\mu < \infty$, then $\sum_n \int f_n d\mu = \int \sum_n f_n d\mu$.
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \sum_n \int |f_n|d\mu &= \sum_n \int f_n^+d\mu + \sum_n \int f_n^-d\mu\\
    &\Rightarrow \sum_n \int f_n^+d\mu<\infty, \sum_n \int f_n^-d\mu < \infty
\end{align*}
By the result of (1.5.6),
\begin{align*}
    \sum_n \int f_n^+d\mu &= \int \sum_n f_n^+d\mu\\
    \sum_n \int f_n^-d\mu &= \int \sum_n f_n^-d\mu\\
    \sum_n \int f_nd\mu &= \sum_n \int f_n^+ - f_n^-d\mu\\
    &= \int \sum_n f_n^+d\mu - \int \sum_n f_n^-d\mu\\
    &= \int \sum_n f_nd\mu
\end{align*}
\end{proof}

\section{Expected Value}
\begin{ex}
Suppose $\varphi$ is strictly convex, i.e., $>$ holds for $\lambda \in (0,1)$. Show that, under the assumptions of Theorem 1.6.2, $\varphi(EX) = E(\varphi X)$ implies $X = EX$ a.s.
\end{ex}
\begin{proof}[sol]
Let $t = EX$. Since $\varphi$ is strictly convex, we can find $a$ which satisfies, for all $x$, $\varphi(x) \ge \varphi(t) + a(x -t)$. Thus $E\varphi(X) \ge \varphi(t) + E[a(X - t)] = \varphi(EX)$. If we add assumption $\varphi(EX) = E(\varphi X)$, then we can conclude $a(X - t) = 0$ a.e. Thus, $X = t = EX$ a.e. 
\end{proof}
\begin{ex}
Suppose $\varphi : \mathbb{R}^n \to \mathbb{R}$ is convex. Imitate the proof of Theorem 1.5.1 to show
\[E \varphi(X_1,...,X_n) \ge \varphi(EX_1,...,EX_n)\]
provided $E|\varphi(X_1,...,X_n)| < \infty$ and $E|X_i| < \infty$ for all $i$.
\end{ex}
\begin{proof}[sol]
Let $c = (EX_1,...,EX_n)$ and define $l(x) = a'(x-c) + \varphi(c),\  a,x \in \mathbb{R}^n$. Then we can find $a$ which satisfies that for all $x$, $\varphi(x) \ge l(x)$ since $\varphi$ is convex. Thus,
\[E\varphi(X_1,...,X_n) \ge aE(X - c) + E\varphi(c) = \varphi(c) = \varphi(EX_1,...,EX_n)\]
\end{proof}
\begin{ex}[Chebyshev's inequality is and is not sharp]
(i) Show that Theorem 1.6.4 is sharp by showing that if $0 < b \le a$ are fixed there is an $X$ with $EX^2 = b^2$ for which $P(|X| \ge a) = b^2/a^2$. (ii) Show that Theorem 1.6.4 is not sharp by showing that if $X$ has $0 < EX^2 <\infty$ then
\[\lim_{a\to\infty} a^2 P(|X| \ge a)/EX^2 = 0\]
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)]
    \[P(|X| \ge a) = P(X^2 \ge a^2) \le EX^2 /a^2 = b^2/a^2\]
    Since $a,b$ is not arbitrary, we can make the equality hold.
    \item[(ii)]
    \begin{align*}
        a^2P(|X| \ge a) = a^2P(X^2 \ge a^2) = \int_{a^2}^\infty a^2d\mu\le \int_{a^2}^\infty X^2d\mu
    \end{align*}
    Since $EX^2$ is finite,
    \begin{align*}
        EX^2 &= \int X^2d\mu\\
        &=\int_0^{a^2} X^2d\mu + \int_{a^2}^\infty a^2d\mu\\
        &\Rightarrow \lim_{a^2\to\infty}\int_{a^2}^\infty a^2d\mu = 0
    \end{align*}
    Thus, $\lim_{a\to\infty}a^2 P(|X| \ge a) = 0$.
\end{enumerate}
\end{proof}
\begin{ex}[One-sided Chebyshev bound]
(i) Let $a > b > 0, 0 < p < 1$, and let $X$ have $P(X = a) = p$ and $P(X = -b) = 1-p$. Apply Theorem 1.6.4 to $\varphi(x) = (x+b)^2$ and conclude that if $Y$ is any random variable with $EY = EX$ and $Var (Y) = Var (X)$, then $P(Y \ge a) \le p$ and equality holds when $Y = X$.
(ii) Suppose $EY = 0$, $Var(Y) = \sigma^2$, and $a > 0$. Show that $P(Y \ge a) \le \sigma^2 /(a^2 + \sigma^2)$, and there is a $Y$ for which equality holds.
\end{ex}
\begin{ex}[Two nonexistent lower bounds]
Show that : (i) if $\epsilon > 0$, $\inf \{P(|X| > \epsilon) : EX = 0, Var(X) = 1\} = 0$. (ii) if $y \ge 1, \sigma^2 \in (0,\infty)$, $\inf \{P(|X| > y) : EX = 1, Var(X) = \sigma^2\} = 0$
\end{ex}
\begin{proof}[sol]
\begin{enumerate}
    \item[(i)]
    \begin{align*}
        P(|X| > \epsilon) \le E(X^2)/\epsilon^2 = V(X)/\epsilon^2
    \end{align*}
    \item[(ii)]
\end{enumerate}
\end{proof}
\begin{ex}[A useful lower bound]
Let $Y \ge 0$ with $EY^2 <\infty$. Apply the Cauchy-Schwarz inequality to $Y 1_{Y >0}$ and conclude
\[P(Y > 0) \ge (EY)^2 / EY^2\]
\end{ex}
\begin{proof}[sol]
\begin{align*}
    E[Y1_{Y > 0}] &\le \sqrt{EY^2E[1_{Y>0}]}\\
    EY - 0\cdot P(Y = 0) &\le \sqrt{EY^2P(Y >0)}\\
    (EY)^2 &\le EY^2P(Y > 0)\\
    P(Y > 0) &\ge (EY)^2 / EY^2
\end{align*}
\end{proof}
\begin{ex}
Let $\Omega = (0,1)$ equipped with the Borel sets and Lebesgue measure. Let $\alpha \in (1,2)$ and $X_n = n^\alpha 1_{(1/(n+1), 1/n)} \to 0$ a.s. Show that Theorem 1.6.8 can be applied with $h(x) = x$ and $g(x) = |x|^{2/\alpha}$, but the $X_n$ are not dominated by an integrable function.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item $g \ge 0$, $\because 2/\alpha > 1$, $g(x) \to \infty$ as $|x| \to \infty$
    \item $|h(x)|/g(x) = |x|^{(\alpha -2)/2}$, $\because (\alpha - 2) < 1$, $|h(x)|/g(x) \to 0$ as $|x| \to \infty$.
    \item $Eg_n(X) = n^2\int 1_{(1/n+1, 1/n)}d\mu  = n/(n+1) < 1$.
\end{enumerate}
Thus, $Eh(X_n) \to Eh(X)$. However, there is no integrable function which dominates $X_n$ because there is no upper bound for $\sup_n |X_n|$.
\end{proof}
\begin{ex}
Suppose that the probability measure $\mu$ has $\mu(A) = \int_A f(x)dx$ for all $A \in \mathcal{R}$. Use the proof technique of Theorem 1.6.9 to show that for any $g$ with $g \ge 0$ or $\int |g(x)|\mu(dx) < \infty$ we have
\[\int g(x)\mu(dx) = \int g(x)f(x)dx\]
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item Indicator function
    
    Let $g = c1_{A}$. Then,
    \[\int g(x)\mu(dx) = \int_A c\mu(dx) = c\mu(A) = \int_A cf(x)dx = \int g(x)f(x)dx\]
    \item Simple function
    
    Let $g = \sum_{i=1}^n c_i1_{A_i}$. Then, by case 1,
    \[\int g(x)\mu(dx) = \sum_{i=1}^n  c_i\mu(A_i) = \int g(x)f(x)dx\]
    \item Nonnegative function
    
    There is a sequence of simple functions $\{g_n\}$ which satisfies that $g_n \uparrow g$. By monotone convergence theorem,
    \[\int g(x)\mu(dx) = \lim_{n\to\infty} \int g_n(x)\mu(dx) = \lim_{n\to\infty} \int g_n(x)f(x)dx = \int g(x)f(x)dx\]
    
    The proof ends if $g \ge 0$.
    \item Integrable function($g$ is integrable)
    
    Let $g = g^+ - g^-$. Then
    \begin{align*}
        \int g^+(x)\mu(dx) &= \int g^+(x)f(x)dx, \int g^-(x)\mu(dx) = \int g^-(x)f(x)dx\\
        \Rightarrow \int g(x)\mu(dx) &=  \int g^+(x)\mu(dx) - \int g^-(x)\mu(dx)\\
        &=  \int g^+(x)f(x)dx - \int g^-(x)f(x)dx = \int g(x)f(x)dx
    \end{align*}
    
    
\end{enumerate}
\end{proof}
\begin{ex}[Inclusion-exclusion formula]
Let $A_1,A_2,...,A_n$ be events and $A = \cup_{i=1}^n A_i$. Prove that $1_A = 1 - \prod_{i=1}^n (1 - 1_{A_i})$. Expand out the right hand side, then take expected value to conclude
\begin{align*}
    P(\cup_{i=1}^n A_i) = &\sum_{i=1}^n P(A_i) - \sum_{i < j}P(A_i \cap A_j)\\
    &+ \sum_{i < j < k} P(A_i \cap A_j \cap A_k) - ... + (-1)^{n-1}P(\cap_{i=1}^n A_i)
\end{align*}
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \omega \in A &\Leftrightarrow \omega \in \cup_{i=1}^n A_i \Leftrightarrow \omega \not\in \cap_{i=1}^n A_i^C\\
    \Rightarrow 1_A &= 1 - \prod_{i=1}^n(1 - 1_{A_i}) \\
    &= \sum 1_{A_i} - \sum 1_{A_i\cap A_j}+ \dotsb + (-1)^{n-1} \times 1_{\cap_{i=1}^n A_i}
\end{align*}
If we take expectation, we obtain the equation.
\end{proof}
\begin{ex}[Bonferroni inequalities] Let $A_1,A_2,...,A_n$ be events and $A = \cup_{i=1}^n A_i$. Show that $1_A \le \sum_{i=1}^n 1_{A_i}$, etc. and then take expected values to conclude
\begin{align*}
    P(\cup_{i=1}^n A_i) & \le \sum_{i=1}^n P(A_i)\\
    P(\cup_{i=1}^n A_i) & \ge \sum_{i=1}^n P(A_i) - \sum_{i < j}P(A_i \cap A_j)\\
    P(\cup_{i=1}^n A_i) & \le \sum_{i=1}^n P(A_i) - \sum_{i < j}P(A_i \cap A_j) + \sum_{i < j < k}P(A_i \cap A_j\cap A_k)
\end{align*}
In general, if we stop the inclusion exclusion formula after an even (odd) number of sums, we get an lower (upper) bound.
\end{ex}
\begin{proof}[sol]
If $\omega \not\in A$, then $\omega \not\in A_i$ for all $i$. If $\omega \in A$, there is at least 1 $A_i$ s.t. $\omega \in A_i$. Thus, $1_A \le \sum_{i=1}^n 1_{A_i}$.

If we stop the formula after an even number of sums, 
\end{proof}
\begin{ex}
If $E|X|^k < \infty$ then for $0 < j < k$, $E|X|^j < \infty$, and furthermore
\[E|X|^j \le (E|X|^k)^{j/k}\]
\end{ex}
\begin{proof}[sol]
Define $\varphi(x) = x ^{k/j}$. Then $\varphi$ is a convex function. By Jensen's inequality
\begin{align*}
    (E|X|^j)^{k/j} = \varphi(E|X|^j) &\le E\varphi(|X|^j) = E|X|^k\\
    \Rightarrow E|X|^j &\le (E|X|^k)^{j/k}
\end{align*}
\end{proof}
\begin{ex}
Apply Jensen's inequality with $\varphi(x) = e^x$ and $P(X = \log y_m) = p(m)$ to conclude that if $\sum_{m=1}^n p(m) = 1$ and $p(m), y_m > 0$ then
\[\sum_{m=1}^n p(m)y_m \ge \prod_{m=1}^n y_m^{p(m)}\]
When $p(m) = 1/n$, this says the arithmetic mean exceeds the geometric mean.
\end{ex}
\begin{proof}[sol]
\begin{align*}
E\varphi(X) &= \sum_{n=1}^m p(m) \exp(\log y_m) = \sum_{n=1}^m p(m)y_m\\
\varphi(EX) &= \exp(\sum_{n=1}^m p(m)\log y_m) = \prod_{n=1}^m y_m^{p(m)}\\
\end{align*}
By Jensen's inequality,
\[\sum_{m=1}^n p(m)y_m  = E\varphi(X) \ge \varphi(EX) = \prod_{m=1}^n y_m^{p(m)}\]
If $p(m) = 1/n$, the left hand becomes arithmetic mean and the right hand becomes geometric mean.
\end{proof}
\begin{ex}
If $EX_1^- <\infty$ and $X_n \uparrow X$ then $EX_n \uparrow EX$.
\end{ex}
\begin{proof}[sol]
Same with Exercise 1.5.5. We use the probability measure in this exercise.
\end{proof}
\begin{ex}
Let $X \ge 0 $ but do NOT assume $E(1/X) < \infty$. Show
\[\lim_{y\to\infty} yE(1/X; X>y) = 0,\quad \lim_{y\downarrow 0} yE(1/X; X>y) = 0\]
\end{ex}
\begin{proof}[sol]
\begin{align*}
    0 \le yE(1/X;X>y) &= \int_{\{\omega : X(\omega) > y\}} y/X d\mu\\
    &\le \int_{\{\omega : X(\omega) > y\}} 1d\mu\\
    &= \mu(\{\omega : X(\omega) > y\})\\
    \Rightarrow\lim_{y\to\infty} yE(1/X; X>y) &\le \lim_{y\to\infty}\mu(\{\omega : X(\omega) > y\}) = 0
\end{align*}
\end{proof}
\begin{ex}
If $X_n \ge 0$ then $E(\sum_{n=0}^\infty X_n) = \sum_{n=0}^\infty EX_n$.
\end{ex}
\begin{proof}[sol]
Same with Exercise 1.5.10.  We use the probability measure in this exercise.
\end{proof}
\begin{ex}
If $X$ is integrable and $A_n$ are disjoint sets with union $A$ then
\[\sum_{n=0}^\infty E(X; A_n) = E(X; A)\]
i.e., the sum converges absolutely and has the value on the right.
\end{ex}
\begin{proof}[sol]
Define $Y_n := \sum_{i=0}^n X1_{A_i}$ and $Y:= X1_A$. Then, $Y_n \to Y$ and $|Y| \le |X|$. Since $X$ is integrable, $Y$ is also integrable. For any $\omega \in \cup_{i=0}^n A_i$, $Y_n(\omega) = Y(\omega)$. Thus, $|Y_n| \le |Y|$. By dominated convergence theorem,
\[\lim_{n\to\infty} EY_n  \to EY \]
\begin{align*}
    \lim_{n\to\infty} EY_n &= \lim_{n\to\infty} E(X;A_n)\\
    EY &= E(X;A)
\end{align*}
\end{proof}
\section{Product Measures, Fubini's Theorem}
\begin{ex}
If $\int_X\int_Y |f(x,y)|\mu_2(dy)\mu_1(dx) < \infty$ then
\[\int_X\int_Y |f(x,y)|\mu_2(dy)\mu_1(dx) = \int_{X\times Y}fd(\mu_1 \times \mu_2) =\int_Y\int_X |f(x,y)|\mu_1(dx)\mu_2(dy)\]
{\bf{Corollary}}. Let $X = \{1,2,...\}$, $\mathcal{A} = $ all subsets of $X$, and $\mu_1 = $ counting measure. If $\sum_n\int |f_n|d\mu < \infty$ then $\sum_n \int f_nd\mu = \int\sum_n f_nd\mu$.
\end{ex}
\begin{ex}
Let $g \ge 0$ be a measurable function on $(X,\mathcal{A},\mu)$. Use Theorem 1.7.2 to conclude that
\[\int_X gd\mu = (\mu \times \lambda)(\{(x,y) : 0 \le y < g(x)\}) = \int_0^\infty \mu(\{x : g(x) > y\})dy\]
\end{ex}
\begin{align*}
    \int_X gd\mu &= \int_X\int_\mathbb{R} 1_{0 \le y < g(x)}dyd\mu\\
    &= (\mu \times \lambda) (\{(x,y) : 0 \le y < g(x)\}\\
    &= \int_\mathbb{R}\int_X 1_{g(x) > y \ge 0}d\mu dy\\
    &= \int_\mathbb{R} \mu(\{x : g(x) > y \ge 0\})dy\\
    &= \int_0^\infty \mu(\{x : g(x) > y\})dy
\end{align*}
\begin{ex}
Let  $F, G$ be Stieltjes measure functions and let $\mu, \nu$ be the corresponding measures on $(\mathbb{R}, \mathcal{R})$. Show that
\begin{enumerate}
    \item[(i)] $\int_{(a,b]} \{F(y) - F(a)\}dG(y) = (\mu\times\nu)(\{(x,y):a<x\le y\le b\})$
    \item[(ii)] $\int_{(a,b]}F(y)dG(y) + \int_{(a,b]}G(y)dF(y) = F(b)G(b) - F(a)G(a) + \sum_{x\in(a,b]}\mu(\{x\})\nu(\{x\})$
    \item[(iii)] If $F =G$ is continous then $\int_{(a,b]}2F(y)dF(y) = F^2(b) - F^2(a)$.
\end{enumerate}
To see the second term in (ii) is needed, let $F(x) = G(x) = 1_{[0,\infty)}(x)$ and $a < 0 < b$.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] 
    \begin{align*}
        \int_{(a,b]} \{F(y) - F(a)\}dG(y) &= \int_{(a,b]}\int_{(a,y])}1dF(x)dG(y)\\ 
        &= \int_{(a,y]\times (a,b]}1d(\mu\times\nu)\\
        &= (\mu\times\nu)(\{(x,y):a<x\le y\le b\})
    \end{align*}
    \item[(ii)] By (i),
    \begin{align*}
        &\ \int_{(a,b]}F(y)dG(y) = (\mu\times\nu)(\{(x,y):a<x\le y\le b\}) + F(a)(G(b)-G(a)) \\
        &\ \int_{(a,b]}G(y)dF(y) = (\mu\times\nu)(\{(x,y):a<y\le x\le b\}) + G(a)(F(b)-F(a)) \\
        &\Rightarrow \int_{(a,b]}F(y)dG(y) + \int_{(a,b]}G(y)dF(y)\\
        &= (\mu\times\nu)(\{(x,y):a<x,y\le b\}) + F(a)(G(b)-G(a)) + G(a)(F(b)-F(a))\\
        &= F(b)G(b) - F(a)G(a) + \sum_{x\in(a,b]}\mu(\{x\})\nu(\{x\})
    \end{align*}
    
    \item[(iii)] By (ii),
        \[\int_{(a,b]}2F(y)dF(y) = F^2(b) - F^2(a) + \sum_{x\in(a,b]}\mu(\{x\})^2\]
    Since $F$ is continuous, $\mu(\{x\}) = 0$ for all $x \in (a,b]$.
\end{enumerate}
\end{proof}
\begin{ex}
Let $\mu$ be a finite measure on $\mathbb{R}$ and $F(X) = \mu((-\infty, x])$. Show that
\[\int (F(x+c) - F(x)) dx = c\mu(\mathbb{R})\]
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \int (F(x+c) - F(x))dx &= \int \mu((x,x+c])dx\\
    &=\int\int 1_{(x,x+c]}(y) d\mu dx\\
    &=\int\int 1_{[y-c,y)}(x) dx d\mu\\
    &= \int cd\mu = c\mu(\mathbb{R})
\end{align*}
\end{proof}
\begin{ex}
Show that $e^{-xy}\sin x$ is integrable in the strip $0 < x < a$, $0 < y$. Perform the double integral in the two orders to get:
\[\int_0^a \frac{\sin x}{x}dx = \arctan(a) - (\cos a)\int_0^\infty \frac{e^{-ay}}{1+y^2}dy - (\sin a)\int_0^\infty \frac{ye^{-ay}}{1+y^2}dy\]
and replace $1+y^2$ by $1$ to conclude $|\int_0^a (\sin x)/x dx - \arctan(a)| \le 2/a$ for $a \ge 1$. 
\end{ex}


\chapter{Law of Large Numbers}
\section{Independence}
\begin{ex}
Suppose $(X_1,...,X_n)$ has density $f(x_1, x_2,...,x_n)$, that is 
\[P((X_1,X_2,...,X_n) \in A) = \int_A f(x) dx \text{ for } A \in \mathcal{R}^n\]
If $f(x)$ can be written as $g_1(x_1)\dotsb g_n(x_n)$ where the $g_m \ge 0$ are measurable, then $X_1,X_2,...,X_n$ are independent. Note that the $g_m$ are not assumed to be probability densities.
\end{ex}
\begin{proof}[sol]
Since $f$ is integrable, $g_i$s are integrable. If we set $f_i(x) := g_i(x) / \int g_i(x)dx$, then $f_i$s are probability densities. Thus, $X_1,X_2,...X_n$ are independent.
\end{proof}
\begin{ex}
Suppose $X_1,...,X_n$ are random variables that take values in countable sets $S_1,...,S_n$. Then in order for $X_1,...,X_n$ to be independent, it is sufficient that whenever $x_i \in S_i$
\[P(X_1 = x_1,...,X_n = x_n) = \prod_{i=1}^n P(X_i = x_i)\]
\end{ex}
\begin{proof}[sol]
We can make $\sigma$-algebra by singletons. Thus, it is sufficient to show that above equality holds.
\end{proof}
\begin{ex}
Let $\rho(x, y)$ be a metric. (i) Suppose $h$ is differentiable with $h(0) =
0$, $h'(x) > 0$ for $x > 0$ and $h'(x)$ decreasing on $[0,\infty)$ Then $h(\rho(x, y))$ is a metric. (ii) $h(x) = x/(x + 1)$ satisfies the hypotheses in (i).
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)]
    $h(\rho(x,x)) = h(0) = 0$.
    
    $h(\rho(x,y)) = h(\rho(y,x))$
    
    Suppose, $\rho(x,y) \le \rho(x,z) + \rho(y,z)$.
    \begin{align*}
        h(\rho(y,z)+\rho(x,z)) - h(\rho(x,z)) &= \int_{\rho(x,z)}^{\rho(y,z)+\rho(x,z)} h'(x)dx\\
        &<\int_0^{\rho(y,z)} h'(x)dx = h(\rho(y,z))\\
        h(\rho(y,z)+\rho(x,z)) &< h(\rho(y,z))+h(\rho(x,z))
    \end{align*}
    Thus,
    \[h(\rho(x,y)) < h(\rho(y,z))+h(\rho(x,z))\]
    \item[(ii)]
    $h(0) = 0$
    
    $h'(x) = 1/(x+1)^2 > 0$. Thus $h'(x)$ is decreasing.
\end{enumerate}
\end{proof}
\begin{ex}
Let $\Omega = (0, 1)$, $\mathcal{F}$ = Borel sets, $P$ = Lebesgue measure. $X_n(\omega) = \sin(2\pi n\omega)$, $n = 1, 2,...$ are uncorrelated but not independent.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item Uncorrelated
    For $n < m$,
    \begin{align*}
        E X_nX_m &= \int X_nX_mdP = \int_0^1 \sin(2\pi nx) \sin(2\pi mx)dx\\
        &= \frac{1}{2}\bigg(\frac{\sin(2\pi(m-n)x)}{m-n} - \frac{\sin(2\pi(m+n)x)}{m+n}\bigg)\bigg|_0^1\\
        &= 0
    \end{align*}
    \begin{align*}
        EX_n = \int_0^1 \sin(2\pi nx)dx = 0
    \end{align*}
    Thus, $EX_nX_m - EX_nEX_m = 0$ for all $n \ne m$.
    \item Not Independent
        \[P(X_4 > 0 |X_2 > 0) = 1 \ne P(X_4 > 0) = 1/2\]
\end{enumerate}

\end{proof}
\begin{ex}
(i) Show that if $X$ and $Y$ are independent with distributions $\mu$ and $\nu$ then
\[P(X + Y = 0) = \sum_y \mu(\{ -y\})\nu(\{y\})\]
(ii) Conclude that if $X$ has continuous distribution $P(X = Y ) = 0$.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)]
    \begin{align*}
        P(X+Y = 0) &= P(\cup_y (\{X = -y\}\cap\{Y = y\}))\\
        &= \sum_y P(\{X = -y\}\cap\{Y = y\})\\
        &= \sum_y P(\{X = -y\})P(\{Y = y\}) = \sum_y \mu(\{ -y\})\nu(\{y\})
    \end{align*}
    \item[(ii)]
    Since $X$ is continuous, finite sets has measure 0. Thus,
    \[P(X + Y = 0) = \sum_y \mu(\{ -y\})\nu(\{y\}) = 0\]
\end{enumerate}
\end{proof}
\begin{ex}
Prove directly from the definition that if $X$ and $Y$ are independent and $f$ and $g$ are measurable functions then $f(X)$ and $g(Y)$ are independent.
\end{ex}
\begin{proof}[sol]
Choose $A,B \in \mathcal{R}$.
\begin{align*}
    P(f(X)\in A, g(Y)\in B) &= P(X\in f^{-1}(A), Y \in g^{-1}(B))\\
    &= P(X\in f^{-1}(A))P(Y \in g^{-1}(B))\\
    &= P(f(X)\in A)P(g(Y) \in B)
\end{align*}
Since $f,g$ are measurable, $f^{-1}(A) \in \sigma(X), g^{-1}(B) \in \sigma(Y)$
\end{proof}
\begin{ex}
Let $K \ge 3$ be a prime and let $X$ and $Y$ be independent random variables that are uniformly distributed on $\{0, 1, . . . ,K-1\}$. For $0 \le n < K$, let $Z_n = X + nY \mod K$. Show that $Z_0,Z_1, ...,Z_{K-1}$ are pairwise independent, i.e., each pair is independent. They are not independent because if we know the values of two of the variables then we know the values of all the variables.
\end{ex}
\begin{proof}[sol]
\begin{align*}
    P(Z_n(\omega_1) = A, Z_m(\omega_2) = B) &= K^{-2}\sum_i\sum_j 1_{Z_n(\omega_1) = i}1_{Z_m(\omega_2) = j}\\
    &= K^{-2}1_{\{Z_n(\omega_1) = A\}}1_{\{Z_m(\omega_2) = B\}}\\
    &= K^{-1}1_{\{Z_n(\omega_1) = A\}}K^{-1}1_{\{Z_m(\omega_2) = B\}}
\end{align*}
\end{proof}
\begin{ex}
Find four random variables taking values in $\{-1, 1\}$ so that any three are independent but all four are not.
\end{ex}
\begin{proof}[sol]
 Let $X_1,X_2,X_3,X_4$ be independent random variables with $P(X_i = 1) = P(X_i = -1) = 1/2$. Define $Y_1 = X_1X_2, Y_2 = X_2X_3, Y_3 = X_3X_4, Y_4 = X_4X_1$. Then $Y_1Y_2Y_3Y_4  = 1$. Thus all four random variables are not independent. However, all three random variables are independent.
\end{proof}
\begin{ex}
Let $\Omega = \{1,2,3,4\}$, $\mathcal{F} = $ all subsets of $\Omega$, and $P(\{i\}) = 1/4$. Give an example of two collections of sets $\mathcal{A}_1$ and $\mathcal{A}_2$ that are independent but whose generated $\sigma$-fields are not 
\end{ex}
\begin{proof}[sol]~
Let $\mathcal{A}_1 = \{\{1,2\}\}, \mathcal{A}_2 = \{\{1,3\}, \{1,4\}\}$/
\end{proof}
\begin{ex}
Show that if $X$ and $Y$ are independent, integer-valued random variables, then
\[P(X + Y = n) = \sum_m P(X = m)P(Y = n-m)\]
\end{ex}
\begin{proof}[sol]
We can apply ex 2.1.5. (i).
\end{proof}
\begin{ex}
In Example 1.6.13, we introduced the Poisson distribution with parameter $\lambda$, which is given by $P(Z = k) = e^{-\lambda}\lambda^k / k!$ for $k = 0,1,2,...$. Use the previous exercise to show that if $X$ = Poisson($\lambda$) and $Y$ = Poisson($\mu$) are independent then $X + Y$ = Poisson($\lambda + \mu$).
\end{ex}
\begin{proof}[sol]
Let $Z = X+Y$.
\begin{align*}
    P(Z = n) &= \sum_m P(X = m)P(Y = n-m)\\
    &=\sum_m e^{-\lambda}\lambda^m / m! e^{-\mu}\mu^{n-m}/(n-m)!\\
    &=e^{-\lambda-\mu}\mu^n \sum_m \frac{\lambda^m \mu^{-m}}{m!(n-m)!}\\
    &=e^{-\lambda-\mu}\mu^n\bigg(1 + \frac{\lambda}{\mu}\bigg)^n/n!\\
    &=e^{-\lambda-\mu}(\lambda + \mu)^n/n!
\end{align*}
Thus $X + Y$ = Possion($\lambda + \mu$).
\end{proof}
\begin{ex}
$X$ is said to have a Binomial$(n,p)$ distribution if
\[P(X = m) = \binom{n}{m}p^m(1-p)^{n-m}\]
(i)Show that if $X = $ Binomial($n,p$) and $Y = $ Binomial($m,p$) are independent then $X+Y$ = Binomial($n+m,p$). (ii) Look at Example 1.6.12 and use induction to conclude that the sum of $n$ independent Bernoulli($p$) random variables is Binomial($n,p$).
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] 
    Let $Z = X+Y$.
\begin{align*}
    P(Z = k) &= \sum_i P(X = i)P(Y = k-i)\\
    &=\sum_i \binom{n}{i}p^n(1-p)^{n-i}\binom{m}{k-i}p^{k-i}(1-p)^{m+i-k}\\
    &=p^k(1-p)^{n+m-k}\sum_i\binom{n}{i}\binom{m}{k-i}\\
    &=\binom{n+m}{k}p^k(1-p)^{n+m-k}
\end{align*}
Thus $X + Y$ = Binomial($n+m, p$).
    \item[(ii)]
    \begin{enumerate}
        \item Bernoulli($p$) $\equiv$ Binomial ($1, p$)
        \item Suppose sum of $n$ independent Bernoulli($p$) random variables is Binomial($n,p$).
        
        By (i) and (ii)-(a), then sum of $n+1$ independent Bernoulli($p$) random variables is Binomial($n+1, p$).
    \end{enumerate}
\end{enumerate}
\end{proof}
\begin{ex}
It should not be surprising that the distribution of $X+Y$ can be $F*G$ without the random variables being independent. Suppose $X,Y \in \{0,1,2\}$ and take each value with probability $1/3$. (a) Find the distribution of $X+Y$ assuming $X$ and $Y$ are independent. (b) Find all the joint distributions $(X,Y)$ so that the distribution of $X+Y$ is the same as the answer to (a).
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(a)] 
        \begin{align*}
        P(X+Y = 0) &= P(\{X = 0\}\cap\{Y = 0\}) =P(\{X = 0\})P(\{Y = 0\}) = 1/9\\
        P(X+Y = 1) &= \sum_{i\in\{0,1\}} P(\{X = i\}\cap\{Y = 1-i\})= \sum_i P(\{X = i\})P(\{Y = 1-i\}) = 2/9\\
        P(X+Y = 2) &= \sum_{i\in\{0,1,2\}} P(\{X = i\}\cap\{Y = 2-i\})= \sum_i P(\{X = i\})P(\{Y = 2-i\}) = 3/9\\
        P(X+Y = 3) &=\sum_{i\in\{1,2\}} P(\{X = i\}\cap\{Y = 3-i\})= \sum_i P(\{X = i\})P(\{Y = 3-i\}) = 2/9\\
        P(X+Y = 4) &= P(\{X = 2\}\cap\{Y = 2\}) P(\{X = 2\})P(\{Y = 2\}) = 1/9
        \end{align*}
    \item[(b)] Let $P(X = 0, Y = 1) := p$. Use sum of each column and row is 1/3.
    
    \begin{table}[h]
    \centering
	\begin{tabular}{c|ccc}
	 $X, Y$& 0 & 1 & 2 \\ \hline
	 0&  1/9&  p& 2/9 - p \\
	 1& 2/9 - p &  1/9 &  p\\
	 2&  p& 2/9 - p &1/9 
	\end{tabular}
	\end{table}
\end{enumerate}
\end{proof}
\begin{ex}
Let $X,Y \ge 0$ be independent with distribution functions $F$ and $G$. Find the distribution function of $XY$.
\end{ex}
\begin{proof}[sol]
Let $H$ be the distribution function of $XY$.
\begin{align*}
    H(z) &= P(XY \le z)\\
    &=\int_0^\infty\int_0^\infty 1_{\{XY \le z\}} dFdG\\
    &=\int_0^\infty\int_{0}^{z/y}1dFdG\\
    &=\int_0^\infty F(z/y)dG
\end{align*}
\end{proof}
\begin{ex}
If we want an infinite sequence of coin tossings, we do not have to use Kolmogorov's theorem. Leg $\Omega$ be the unit interval $(0,1)$ equipped with the Borel sets $\mathcal{F}$ and Lebesgue measure $P$. Let $Y_n(\omega) = 1$ if $[2^n\omega]$ is odd and 0 if $[2^n\omega]$ is even. Show that $Y_1,Y_2,...$ are independent with $P(Y_k = 0) = P(Y_k= 1) = 1/2$.
\end{ex}
\begin{proof}[sol]
Let $i_n \in \{0,1\}, n \in\N$. Define $x = \sum_{j =1}^m i_j2^{-j}$. It is trivial that for each $j$, $P(Y_j = 1) = P(Y_j = 0) = 1/2$. For arbitrary $m \in \N$
\begin{align*}
	P(Y_1 = i_1,...,Y_m = i_m) = P(\omega \in [x, x + 2^{-m})) = 2^{-m}
\end{align*}
Therefore,  $Y_1,Y_2,...$ are independent 
\end{proof}
\section{Weak Laws of Large Numbers}
\begin{ex}
Let $X_1,X_2,...$ be uncorrelated with $EX_i = \mu_i$ and $var(X_i)/i \to 0$ as $i \to \infty$. Let $S_n = X_1+\dotsb +X_n$ and $\nu_n = ES_n /n$ then as $n\to\infty$, $S_n/n - \nu_n \to 0$ in $L^2$ and in probability.
\end{ex}
\begin{proof}[sol]
Choose a positive number $\epsilon$. For given $\epsilon$, there exists $\delta_i$ such that
\[Var(X_i) < \delta_i + i\epsilon\]
Let $\delta = \max\{\delta_1,...,\delta_n\}$. Then
\[\sum Var(X_i) < \sum\delta_i + \epsilon\sum i< n\delta + n^2\epsilon\]
\begin{align*}
    E(S_n / n - \nu_n)^2 &= \frac{1}{n^2}\sum Var(X_i)\\
    &< \frac{1}{n}\delta + \epsilon
\end{align*}
Thus, $E(S_n / n - \nu_n)^2 \to 0$ as $n \to \infty$ since $\epsilon$ is arbitrary positive.
\begin{align*}
    E(S_n / n - \nu_n)^2 \to 0 &\Rightarrow P((S_n / n - \nu_n)^2 > \epsilon^2 ) \to 0\\
    &\Rightarrow P(|(S_n / n - \nu_n)| > \epsilon) \to 0
\end{align*}
\end{proof}
\begin{ex}
The $L^2$ weak law generalizes immediately to certain dependent sequences. Suppose $EX_n = 0$ and $EX_nX_m \le r(n-m)$ for $m \le n$ (no absolute value one the left-had side!) with $r(k) \to \infty$ as $k \to \infty$. Show that $(X_1+\dotsc+X_n) / n \to 0$ in probability.
\end{ex}
\begin{proof}[sol]
$L^2$ convergence implies convergence in probability. Thus, it is sufficient to show that the sequences converges in $L^2$.

By Cauchy-Schwarz, 
\[EX_nX_m \le (EX_n^2EX_m^2)^{0.5} = EX_n^2 = r(0)\]
Thus, for all $i$, $r(i) \le r(0)$.
Choose a positive number $\epsilon$. For given $\epsilon$, there exists $K$ such that for all $k > K$, $|r(k)| < \epsilon$. For $n > K$,
\begin{align*}
    E((X_1+\dotsc+X_n) / n)^2 \le \frac{1}{n^2} n(2K+1)r(0) + n^2\epsilon  \le \frac{2K+1}{n}r(0) + \epsilon
\end{align*}
Since $\epsilon$ is arbitrary positive, the sequence converges to 0 in $L^2$.
\end{proof}
\begin{ex}[Monte Carlo integration]
(i) $f$ be a measurable function on $[0,1]$ with $\int_0^1 |f(x)|dx < \infty$. Let $U_1, U_2,...$ be independent and uniformly distributed on $[0,1]$, and let
\[I_n = n^{-1}(f(U_1)+\dotsc+f(U_n))\]
Show that $I_n \to I \equiv \int_0^1 f dx$ in probability. (ii) Suppose $\int_0^1 |f(x)|^2dx < \infty$. Use Chebyshev's inequality to estimate $P(|I_n - I| > a/n^{1/2})$.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] By assumption, $Ef(U_i)$ exists. Thus, 
    \[\sum f(U_i)/n - \int_0^1 fdx \overset{p}{\to} 0 \]
    Thus, the sum converges to $I \equiv \int_0^1 fdx$ in probability.
    \item[(ii)] By Chebyshev's inequality
    \begin{align*}
        P(|I_n - I| > a/n^{1/2}) \le nVar(I_n)/a^2 = Var(f(U_i))/a^2\\
        &= \frac{1}{a^2}\bigg[\int_0^1 f^2dx - \bigg(\int_0^1 fdx\bigg)\bigg]
    \end{align*}
\end{enumerate}
\end{proof}
\begin{ex}
Let $X_1, X_2,...$ be i.i.d. with $P(X_i = (-1)^k k) = C/k^2\log k$ for $k \ge 2$ where $C$ is chosen to make the sum of the probabilities = 1. Show that $E|X_i| = \infty$, but there is a finite constant $\mu$ so that $S_n/n \to \mu$ in probability.
\end{ex}
\begin{proof}[sol]
$E|X_i| = \infty$
\[E|X_i| = \sum^\infty_{k=2} C/k\log k\]
Thus, we can check convergence by whether the below integral converges
\begin{align*}
    \int_2^\infty C/x\log xdx = C\log\log x|_2^\infty = \infty
\end{align*}
Thus, $E|X_i| = \infty$
\begin{align*}
    P(|X| > n) &= \sum_{k = n + 1}^\infty  C/k^2\log k \le C/\log n\int_n^\infty 1/n^2 = C/n^2\log n\\
     \Rightarrow nP(|X| > n) &\le  C/n\log n \to 0 \text{ as } n \to \infty. 
\end{align*}
Thus, $S_n / n$ converges $\lim EX_i 1_{\{X_i \le n\}}$.
\begin{align*}
    \lim EX_i 1_{\{X_i \le n\}} &= \lim \sum^n_{k=2} (-1)^k C/k\log k\\
    &= \sum^\infty_{k=2} (-1)^k C/k\log k
\end{align*}
The series converges since $C/k\log k$ is decreasing and converges to 0.
\end{proof}
\begin{ex}
Let $X_1,X_2,...$ be i.i.d with $P(X_i > x) = e/x\log x$ for $x \ge e$. Show that $E|X_i| = \infty$, but there is a sequece of constants $\mu_n \to \infty$ so that $S_n/n - \mu_n \to 0$ in probability.
\end{ex}
\begin{proof}[sol]
$X_i$ is a non-negative random variable.
\[E|X_i| = EX_i = \int_e^\infty e/x\log xdx = e\log\log x|^\infty_e = \infty\]
\[xP(X_i > x) = e/\log x \to 0 \text{ as } x \to \infty\]
Thus,
\begin{align*}
    \mu_n &= EX_i1_{\{X_i \le n\}}\\
    &=\int_e^n e/x\log xdx = e\log\log n
\end{align*}
\end{proof}
\begin{ex}(i) Show that if $X\ge 0$ is integer valued $EX = \sum_{n\ge 1}P(X\ge n)$. (ii) Find a similar expression for $EX^2$.
\end{ex}
\begin{proof}[sol]
$X$ is interger valued variable.
\begin{align*}
	X &= \sum_{i=1}^\infty I(X \ge n) \Rightarrow EX = \sum_{i=1}^\infty P(X \ge n)\\
	X^2 &=  \sum_{i=1}^\infty (2n-1)I(X \ge n) \Rightarrow EX^2 = \sum_{i=1}^\infty (2n-1)P(X \ge n)
\end{align*}
\end{proof}
\begin{ex}
Generalize Lemma 2.2.13 to conclude that if $H(x) = \int_{(-\infty, x]} h(y)dy$ with $h(y) \ge 0$, then
\[EH(X) = \int_{-\infty}^\infty h(y)P(X \ge y)dy\]
An important special case is $H(x) = \exp (\theta x)$ with $\theta >0$.
\end{ex}
\begin{proof}[sol]
\begin{align*}
    EH(x) &= \int H(X)dP = \int\int_{(-\infty, x]} h(y)dydP\\
    &= \int\int_{[y,\infty)}h(y)dPdy\\
    &= \int h(y)P(X \ge y)dy
\end{align*}
\end{proof}
\begin{ex}[An unfair ``fair game''] Let $p_k  = 1/2^kk(k+1), k = 1,2,...$ and $p_0 = 1 -\sum_{k\ge 1}p_k$.
\[\sum_{k=1}^\infty 2^kp_k = (1 - 1/2) + (1/2 - 1/3) + \dotsb = 1\]
so if we let $X=1,X_2,...$ be i.i.d. with $P(X_n = -1) = p_0$ and 
\[P(X_n = 2^k - 1) = p_k\ \text{for} \ k \ge 1\]
then $EX_n = 0$. Let $S_n = X_1 + \dotsb + X_n$. Use Theorem 2.2.11 with $b_n = 2^{m(n)}$ where m(n) = $\min\{m : 2^{-m} m^{-3/2} \le n^{-1}\}$ to conclude that 
\[S_n/(n/\log_2 n) \to -1 \text{ in probability}\]
\end{ex}
\begin{proof}[sol]
\end{proof}
\begin{ex}[Weak law for positive variables] Suppose $X_1,X_2,...$ are i.i.d., $P(0\le X_i < \infty) = 1$ and $P(X_i > x) > 0$ for all $x$. Let $\mu(s) = \int_0^s xdF(x)$ and $\nu(s) = \mu(s)/s(1 -F(s))$. It is known that there exist constatns $a_n$ so that $S_n/a_n \to 1$ in probability, if and only if $\nu(s) \to\infty$ as $s \to \infty$. Pick $b_n \ge 1$ so that $n\mu(b_n) = b_n$ (this works for large $n$), and use Theorem 2.2.11 to prove that the condition is sufficient.
\end{ex}
\begin{proof}[sol]
\end{proof}
\section{Borel-Cantelli Lemmas}
\begin{ex}
Prove that $P(\limsup A_n) \ge \limsup P(A_n)$ and $P(\liminf A_n) \le \liminf P(A_n)$.
\end{ex}
\begin{proof}[sol]
$\liminf A_n = \bigcup_{n\ge 1}\bigcap_{k \ge n}A_k$ and $\limsup A_n = \bigcap_{n\ge 1}\bigcup_{k \ge n}A_k$
\begin{align*}
P(\liminf A_n) &= \lim P(\cap_{k \ge n}A_k) = \liminf P(\cap_{k \ge n}A_k) \le \liminf P(A_n)\\
P(\limsup A_n) &= \lim P(\cup_{k \ge n}A_k) = \limsup P(\cup_{k \ge n}A_k) \ge \limsup P(A_n)
\end{align*}
\end{proof}
\begin{ex}
Prove the first result in Theorem 2.3.4 directly from the definition.
\end{ex}
\begin{proof}[sol]
For arbitrary $\varepsilon > 0$, there exists $\delta > 0$ which implies
\[|X_n - X| < \delta \Rightarrow |f(X_n) - f(X)| < \varepsilon\]
Thus, $X_n \to X$ in probability implies $f(X_n) \to f(X)$ in probability.
\end{proof}
\begin{ex}
Let $\ell_n$ be the length of the head run at time. See Example 8.3.1. for the precise definition. Show that $\limsup_{n\to\infty} \ell_n /\log_2 n = 1$, $\liminf_{n\to\infty} \ell_n = 0$ a.s.
\end{ex}
\begin{ex}[Fatou's lemma]
Suppose $X_n \ge 0$ and $X_n \to X$ in probability. Show that $\liminf_{n\to\infty} EX_n \ge EX$.
\end{ex}
\begin{proof}[sol]
Pick a subsequence $X_{n(m)}$ which satisfies that
\[\lim_{m\to\infty} EX_{n(m)} = \liminf_{n\to\infty} EX_n \]
Since $X_n$ converges in probability, we can find a further subsequence $X_{n(m_k)}$ such that $X_{n(m_k)} \to X$ a.s.

If we apply Fatou's lemma,
\[EX \le \liminf_{k\to\infty} EX_{n(m_k)} \le \liminf_{n\to\infty} EX_n\]

\end{proof}
\begin{ex}[Dominated convergence]
Suppose $X_n \to X$ in probability and (a) $|X_n| \le Y$ with $EY < \infty $ or (b) there is a continuous function $g$ with $g(x) > 0$ for large $x$ with $|x|/g(x) \to 0$ as $|x| \to \infty$ so that $Eg(X_n) \le C < \infty $ for all $n$. Show that $EX_n \to EX$.
\end{ex}
\begin{proof}[sol]
Convergence in probability implies pointwise convergence. Thus, we need a dominating function to apply Dominated Convergence Theorem.
\begin{enumerate}
	\item[(a)] $Y$ dominates $X_n$.
	\item[(b)] Let $Z = g(X)$. Then, $Z$ dominates $X_n$.
\end{enumerate}
Thus, we can apply DCT.
\end{proof}
\begin{ex}[Metric for convergence in probability]
Show (a) that $d(X,Y) = E(|X-Y|/(1+|X-Y|))$ defines a metric on the set of random variables, i.e., (i) $d(X,Y) = 0$ if and only if $X = Y$ a.s., (ii) $d(X,Y) = d(Y,X)$, (iii) $d(X,Z) \le d(X,Y) + d(Y,Z)$ and (b) that $d(X_n, X) \to 0$ as $n \to \infty$ if and only if $X_n \to X$ in probability.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
\item[(a)] 
(i) $X = Y$ a.s. $\Leftrightarrow$ $d(X,Y) = 0$

(ii) $d(X,Y) = E(|X-Y|/(1+|X-Y|)) =  E(|Y-X|/(1+|Y-X|)) = d(Y,X)$

(iii) $|X-Z| = |X -Y + Y -Z| \le |X-Y| + |Y -Z|$. By result of 2.1.3, $d(X,Z) \le d(X,Y) + d(Y,Z)$.

\item[(b)]
($\Rightarrow$)
For given $\epsilon > 0$,
\begin{align*}
P(|X_n - X| > \epsilon) &= P(|X_n-X|/(1+|X_n-X|) > \epsilon/(1+\epsilon))\\
 &< d(X_n,X) (1+\epsilon)/\epsilon \to 0 \quad \because \text{ Chebychev}
\end{align*}
Thus, $X_n \to X$ in probability.

($\Leftarrow$)
$X_n \to X$ in probability and $|d(\cdot, \cdot)| \le 1 $. Thus, $d(X_n,X) \to d(X,X) = 0$ by DCT.
\end{enumerate}
\end{proof}
\begin{ex}
Show that random variables are a complete space under the metric defined in the previous exercise, i.e., if $d(X_m, X_n) \to 0$ whenever $m,n \to \infty$ then there is a r.v. $X_\infty$ so that $X_n \to X_\infty$ in probability.
\end{ex}
\begin{proof}[sol]
Choose $N_k$ so that if $n,m \ge N_k$, then $d(X_n, X_m) \le 2^{-k}$. Pick a subsequence $\{X_{n_k}\}$ which satisfies $n_k > N_k$. Then
\[P(|X_{n_{k+1}} - X_{n_k}| > k^{-2}) \le d(X_{n_{k+1}}, X_{n_k}) \frac{1 + k^{-2}}{k^{-2}} \le 2^{-k}(1+ k^2)\]
The last term is summable so by the Borel Cantelli Lemma, $\limsup P(|X_{n_{k+1}} - X_{n_k}| > k^{-2}) = 0$. Thus, there exists $X$ such that $X_{n_k} \to X$ in probability. Let $X'$ be the limit of another subsequence. By the Cauchy assumption, $d(X_{n_k}, X_{m_k}) \to 0$. The metric is defined as expection. By exercise 2.3.4, $d(X, X') = 0$. Thus the random variable exists.
\end{proof}
\begin{ex}
Let $A_n$ be a sequence of independent events with $P(A_n) < 1$ for all $n$. Show that $P(\cup A_n) = 1$ implies $\sum_n P(A_n) = \infty$ and hence $P(A_n \ i.o.) = 1$.
\end{ex}
\begin{proof}[sol]
By second Borel Cantelli Lemma, it is sufficient to show that $P(\cup A_n) = 1$ implies $\sum_n P(A_n) = \infty$.

Suppose $\sum_n P(A_i)$ is finite. Thus, for some $N$, $P(A_n) = 0$ if $n > N$.
\begin{align*}
P(\cup A_n) = 1 &\Rightarrow P(\cap A_n^c) = 0\\
&\Rightarrow \prod_n [1 - P(A_n)] = 0
\end{align*}
However, $\prod_n [1 - P(A_n)] $ converges to $\prod^N_{n=1} [1 - P(A_n)] > 0$. Thus, there is contradiction.
\end{proof}
\begin{ex}
(i) If $P(A_n) \to 0$ and $\sum_{n = 1}^\infty P(A_n^c \cap A_{n+1}) < \infty$ then $P(A_n, i.o.) = 0$. (ii) Find an example of a sequence $A_n$ to which the result in (i) can be applied but the Borel - Cantelli lemma cannot.
\end{ex}
\begin{proof}[sol]
\begin{enumerate}
	\item[(i)] Let $B_n = A_n^c \cap A_{n+1}$. The partial sum of $P(B_n)$ goes to 0.
	\begin{align*}
	P(\bigcup_{n = m} A_i) &\le P(A_m) + \sum_{n = m} P(B_i) \to 0 \text{ as } n \to \infty\\
	P(\limsup A_n) &= \inf P(\bigcup_{n = m} A_i) = 0
	\end{align*}
	
	\item[(ii)]
	Let $A_n := [0, n^{-1}]$. Then the lemma can not be applied since $\sum P(A_n) = \infty$. However, $P(A_n) \to 0, P(A_n^c \cap A_{n+1}) \to 0$.
\end{enumerate}
\end{proof}
\begin{ex}[Kochen-Stone lemma]
Suppose $\sum P(A_k) = \infty$. Use Exercieses 1.6.6 and 2.3.1 to show that if 
\[\limsup_{n\to\infty} \bigg(\sum_{k=1}^n P(A_k)\bigg)^2 \bigg/ \bigg(\sum_{1\le j,k\le n}P(A_j \cap A_k)\bigg) = \alpha > 0\]
then $P(A_n, i.o.) \ge \alpha$. The case $\alpha = 1$ contains Theorem 2.3.7
\end{ex}
\begin{proof}[sol]
Define $X_n := \sum_{i=1}^n 1\{A_i\}$ Then, $EX_n = \sum_{i=1}^n P(A_i), EX_n^2 = \sum_{1\le i,j \le n}P(A_i \cap A_j)$. By exercise 1.6.6.
\[P(X_n \ge 0) \ge (EX_n)^2/EX_n^2 \Rightarrow \limsup P(X_n \ge 0) \ge \limsup  (EX_n)^2/EX_n^2 = \alpha\]

\end{proof}
\begin{ex}
Let $X_1,X_2,...$ be independent with $P(X_n = 1) = p_n$ and $P(X_n = 0) = 1 - p_n$. Show that (i) $X_n \to 0$ in probability if and only if $p_n \to 0$, and (ii) $X_n \to 0$ a.s. if and only if $\sum p_n < \infty$.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
\item[(i)]
For given $0 <\epsilon < 1$,
\[P(|X_n| > \epsilon) = P(X_n > \epsilon) = P(X_n = 1) = p_n\]
Thus, $X_n \to 0 \Leftrightarrow p_n \to 0$.
\item[(ii)]
($\Rightarrow$)
$X_n \to 0$ a.s. implies, for some $N$, $X_n = 0$ if $n > N$. Thus, $\sum_n p_n = \sum_{n=1}^N p_n <\infty$.

($\Leftarrow$)
$\sum p_n < \infty \Rightarrow$ for some $N$, $p_n = 0$ if $n > N$. Thus $X_n \to 0$ a.s.
\end{enumerate}
\end{proof}
\begin{ex}
Let $X_1,X_2,...$ be a sequence of r.v.'s on $(\Omega, \mathcal{F}, P)$ where $\Omega$ is a countable set and $\mathcal F$ consists of all subsets of $\Omega$. Show that $X_n \to X$ in probability implies $X_n \to X$ a.s.
\end{ex}
\begin{proof}[sol]
Since $\Omega$ is countable, the image of a random variable is countable. And all subsets are measurable by assumption. For every $\epsilon >0 $, 
\[\lim P(|X_n - X| < \epsilon ) = 1\]
Thus, for any $\omega \in \Omega$, $|X_n(\omega) - X(\omega)| < \epsilon$. This implies $X_n \to X$ a.s.
\end{proof}
\begin{ex}
If $X_n$ is any sequence of random variables, there are constants $c_n \to \infty$ so that $X_n / c_n \to 0$ a.s.
\end{ex}
\begin{proof}[sol]
For  every $\epsilon > 0$, choose $m$ s.t. $\epsilon < 2^{-m}$. Choose $c_n$ satisties that $P(X_n > c_n2^{-n}) \le 2^{-n}$. Then $\sum P(X_n / c_n > \epsilon) < \sum P(X_n / c_n > 2^{-m}) < \infty$. Thus, $P(\limsup X_n / c_n > 0) = 0$.
\end{proof}
\begin{ex}
Let $X_1,X_2,...$ be independent. Show that $\sup X_n < \infty$ a.s. if and only if $\sum_n P(X_n > A) < \infty$ for some $A$.
\end{ex}
\begin{proof}[sol]~\
\begin{enumerate}
	\item[($\Rightarrow$)] Let $A > \sup X_n$. Then $\sum P(X_n > A) = 0$.
	\item[($\Leftarrow$)] $\sum P(X_n > A) < \infty \Rightarrow P(\limsup X_n > A) = 0$. Thus $\sup X_n < \infty$ a.s.
\end{enumerate}
\end{proof}
\begin{ex}
Let $X_1, X_2,...$ be i.i.d. with $P(X_i > x)  = e^{-x}$, let $M_n = \max_{1\le m \le n} X_m$. Show that (i) $\limsup_{x\to\infty} X_n / \log n = 1$ a.s. and (ii) $M_n / \log n \to 1$ a.s.
\end{ex}
\begin{proof}[sol]~\
\begin{enumerate}
\item[(i)] $P(X_n \ge \log n) = 1/n \Rightarrow \sum P(X_n \ge \log n) = \infty$. Thus $P(X_n/\log n \ge 1\ i.o.) = 1$.However, for any $\epsilon > 0$, $P(X_n \ge (1 + \epsilon )\log n) = n^{-1-\epsilon}$. Thus the sum of probability is 0. Then $P( X_n/\log n \ge 1 + \epsilon\ i.o.) = 0$.
\item[(ii)] By (i), $M_n \le (1+\epsilon)\log n \Rightarrow \limsup M_n/\log n \le 1$.
\begin{align*}
P(M_n < (1-\epsilon)\log n) &= (1 - n^{-1+\epsilon})^n\\
&\le \exp(-n^\epsilon)
\end{align*}
Thus, by Borel Cantelli, $P(M_n < (1-\epsilon)\log n\ i.o.) = 0 \Rightarrow \liminf M_n/\log n \ge 1$.
\end{enumerate}
\end{proof}
\begin{ex}
Let $X_1,X_2,...$ be i.i.d. with distribution $F$, let $\lambda_n \uparrow \infty$, and let $A_n = \{\max_{1\le m\le n} X_m \lambda_n\}$. Show that $P(A_n \ i.o.) = 0$ or 1 according as $\sum_{n \ge 1} (1 - F(\lambda_n)) < \infty$ or $ = \infty$.
\end{ex}
\begin{proof}
By Borel Cantelli lemma, $\sum_{n \ge 1} (1 - F(\lambda_n)) < \infty$ implies $P(X_n > \lambda_n \ i.o.) = 0$. Thus, $P(A_n\ i.o.) = 0$. Otherwise, $\sum_{n \ge 1} (1 - F(\lambda_n)) = \infty$ implies $P(X_n > \lambda_n \ i.o.) = 1$ by second Borel Cantelli lemma. Thus, $P(A_n\ i.o.) = 1$.
\end{proof}
\begin{ex}
Let $Y_1,Y_2,...$ be i.i.d. Find necessary and sufficient conditions for
\begin{enumerate}
\item[(i)] $Y_n / n\to 0$ almost surely
\item[(ii)] $(\max_{m\le n} Y_m)/n \to 0$ almost surely
\item[(iii)] $(\max_{m\le n} Y_m)/n \to 0$ in probability
\item[(iv)] $Y_n/n \to 0$ in probability
\end{enumerate}
\end{ex}
\begin{ex}
Let $0 \le X_1 \le X_2 ...$ be random variables with $EX_n \sim an^\alpha$ with $a, \alpha > 0$, and var$(X_n) \le Bn^\beta$ with $\beta < 2\alpha$. Show that $X_n / n^\alpha \to a$ a.s.
\end{ex}
\begin{proof}[sol]
Let $Y_n := X_n /n^\alpha$. Then $EY_n \sim \alpha$ and var$(Y_n) \le Bn^{\beta - 2\alpha}$. By Chebyshev,
\[P(|Y_n - a| > \epsilon) \le Var(Y_n)/\epsilon^2\]
Since the right term is summable, $P(|Y_n - a| > \epsilon, \ i.o.) = 1$. Thus $X_nn / n^\alpha \to a$ a.s.
\end{proof}
\begin{ex}
Let $X_n$ be independent Poisson r.v.'s with $EX_n = \lambda_n$, and let $S_n = X_1 + \dotsb + X_n$. Show that if $\sum\lambda_n = \infty$ then $S_n/ES_n \to 1$. a.s.
\end{ex}
\begin{proof}[sol]
Var$(S_n)$ = $\sum$ Var($X_n$) = $\sum \lambda_n$.
\[P(|S_n /ES_n - 1| > \epsilon) \le \epsilon^{-2}/\sum\lambda_n\]
\end{proof}
\begin{ex}
Show that if $X_n$ is the outcome of the $n$th play of the St.Petersberg game (Example 2.2.16) then $\limsup_{n\to\infty} X_n/ (n\log_2 n) = \infty$ a.s. and hence the same result holds for $S_n$. This shows that the convergence $S_n / (n\log_2 n) \to 1$ in probability proved in Section 2.2. does not occur a.s.
\end{ex}

\section{Strong Law of Large Numbers}
\begin{ex}[Lazy janitor]
Suppose the $i$th light bulb burns for an amount of time $X_i$ and then remains burned out for time $Y_i$ before being replaced. Suppose the $X_i, Y_i$ are positive and independent with the $X$’s having distribution $F$ and the $Y$’s having distribution $G$, both of which have finite mean. Let $R_t$ be the amount of time in $[0, t]$ that we have a working light bulb. Show that $R_t/t \to EX_i/(EX_i + EY_i)$ almost surely.
\end{ex}
\begin{proof}[sol]
Let $\underbar R_t = \sum_{n \le N(t)}X_n, \bar R_t = \sum_{n \le N(t) + 1}X_n$ Then, $\underbar R_t \le R_t \le \bar R_t$. SLLN implies
\[\lim \underbar R_t /N(t) = \lim \bar R_t / (N(t)  + 1) = EX_1 \ a.s.\]
Also
\[\lim N(t) / t = \lim (N(t) +1)/t = 1 /(EX_1  +EY_1)\ a.s. \]
Thus,
\[\lim\underbar R_t/t = \lim\bar R_t/t = EX_1  / (EX_1 + EY_1)\
 a.s.\]
Finally
\[R_t/t \to EX_1  / (EX_1 + EY_1)\ a.s.\]
\end{proof}
\begin{ex}
Let $X_0 = (1,0)$ and define $X_n \in \R^2$ inductively by declaring that $X_{n+1}$ is chosen at random from the ball of radius $|X_n|$ centered at the origin, i.e., $X_{n+1}/|X_n|$ is uniformly distributed on the ball of radius 1 and independent of $X_1,...,X_n$. Prove that $n^{-1}\log |X_n| \to c$ a.s. and compute $c$
\end{ex}
\begin{proof}[sol]
By assumption $|X_1|, |X_2|/|X_1|, |X_3|/|X_2|,...$ have are i.i.d. with distribution $U$ which $P(U < u) = u^2$ for $u \in [0.1]$. Thus, $|X_n| = U_1U_2\dotsb U_n$. Then
\[\frac{\log|X_n|}{n} = \frac{\sum_{m=1}^n \log U_m}{n} \to E \log U_m\]
$E\log U_m = \int_0^1 2u\log udu = -1/2$. Thus, $c = -1/2$.
\end{proof}
\begin{ex}[Investment problem]
We assume that at the beginning of each year totu can buy bonds for \$1 that are worth \$ $a$ at the end of the year or stocks that are worth a random amount $V \ge 0$. If you always invest a fixed proportion $p$ of yout wealth in bonds, then your wealth at the end of year $n+1$ is $W_{n+1} = (ap + (1-p)V_n)W_n$. Suppose $V_1,V_2,...$ are i.i.d. with $EV_n^2 < \infty$ and $E(V_n^{-2}) < \infty$. (i) Show that $n^{-1}\log W_n \to c(p)$ a.s. (ii) Show that $c(p)$ is concave. (iii) By investigating $c'(0)$ and $c'(1)$, give conditions on $V$ that guarantee that the optimal choice of $p$ is in (0,1). (iv) Suppose $P(V = 1) = P(V = 4) = 1/2$. Find the optimal $p$ as a function of $a$.
\end{ex}
\begin{proof}[sol]
\begin{align*}
\log W_{n+1} &= \log W_n + \log (ap + (1-p)V_n)\\
&=\sum_{i=1}^n \log(ap + (1-p)V_b\\
\intertext{By SLLN}
\frac{\log W_n}{n} &\to E\log(ap + (1-p) V_1) \equiv c(p)
\end{align*}
Concavity of $c(p)$
\begin{align*}
c'(p) &=  E\bigg(\frac{a - V_1}{ap + (1-p)V_1}\bigg)\\
c''(p) &= -E\bigg(\frac{((a-V_1)^2}{ap + (1-p)V_1)^2}\bigg)
\end{align*}

$c(p)$ is concave since $c$ is twice differentiable and second derivative is negative.

To have a maximum in (0,1), $c'(0) > 0$ and $c'(1) < 0$. Thus,
\[aEV_1^{-1} > 1, \ EV_1 > a\]

Using above condition, optimal $p$ lies in (0,1) when $8/5 <a < 5/2$. Optimal $p$ satisfies $c'(p) = 0$. Thus, we can obtain that
\[p^* = \frac{5a - 8}{-2a^2 +6a - 8}\]

If $a \le 8/5$, then $p^* = 0$ and if $a \ge 5/2$, then $p^* =1$
\end{proof}

\section{Convergence of Random Series$^*$}
\begin{ex}
Suppose $X_1,X_2,...$ are i.i.d. with $EX_i = 0$, var$(X_i) = C <\infty$. Use Theorem 2.5.5 with $n = m^\alpha$ where $\alpha(2p - 1) > 1$ to conclude that if $S_n = X_1 + \dotsb + X_n$ and $p > 1/2$ then $S_n/n^p \to 0$ almost surely.
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}
\begin{ex}
\end{ex}

\section{Renewal Theory$^*$}
\section{Large Deviations$^*$}

\chapter{Central Limit Theorems}
\section{The De Moivre-Laplace Theorem}
\section{Weak Convergence}
\section{Characteristic Functions}
\section{Centrel Limit Theorems}
\section{Local Limit Theorem$^*$}
\section{Poisson Processes}
\section{Stable Laws$^*$}
\section{Infinitely Divisible Distributions$^*$}
\section{Limit Theorems in $\R^d$}
\end{document}













































