\documentclass{report}

\usepackage{makecell}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathrsfs}
%\usepackage[hangul]{kotex}
%\usepackage{kotex-logo}
\usepackage{float}
\usepackage[left=2.5cm,right=2.5cm,top=3cm,bottom=3cm,a4paper]{geometry}
\usepackage{fancyhdr}
\usepackage{indentfirst}


\newtheorem{ex}{}[section]


\makeatletter

\newcommand\frontmatter{%
    \clearpage
  \pagenumbering{roman}}

\newcommand\mainmatter{%
    \clearpage
  \pagenumbering{arabic}}

\newcommand\backmatter{%
  \if@openright
    \cleardoublepage
  \else
    \clearpage
  \fi
   }
   
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\footrulewidth}{0.5pt}
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\thepage}
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.5pt}
}




\begin{document}


\frontmatter

\mainmatter

\chapter{Measure Theory}
\section{Probability Spaces}
\begin{ex}
Let $\Omega = \mathbb{R}, \mathcal{F} =$ all subsets so that $A$ or $A^c$ is countable, $P(A) = 0$ in the first case and = 1 in the second. Show that $(\Omega, \mathcal{F}, P)$ is a probability space.
\end{ex}
\begin{proof}[sol]
i) $\mathcal{F}$ is a $\sigma-$algebra on $\mathbb{R}$.

$\emptyset \in \mathcal{F}$ since $\emptyset$ is countable.

By definition, $\mathcal{F}$ is closed under complementations.

Countable union of countable sets is countable.  Union of countable sets and uncountable sets is uncountable. Thus, $\mathcal{F}$ is closed under countable union. 

ii) $P$ is a probability measure.

$P(\emptyset) = 0$ since $\emptyset$ is countable. By definition, for any set $A$, $P(A) \ge 0$.

Countable union of countable sets is countable.  Union of countable sets and uncountable sets is uncountable. Thus, $P$ has the countable additivity property. 

If $A$ is countable, then $A^c$ is uncountable since $\Omega$ is uncountable. Thus,
\[P(\Omega) = P(A) + P(A^c) = 1\]
By (i) and (ii), $(\Omega, \mathcal{F}, P)$ is a probability space.
\end{proof}
\begin{ex}
Recall the definition of $\mathcal{S}_d$ from Example 1.1.5. Show that $\sigma (\mathcal{S}_d) =\mathcal{R}^d$, the Borel subsets of $\mathbb{R}^d$
\end{ex}
\begin{proof}[sol]
i) $d = 1$. In $\mathbb{R}$, any open set can be represented by countable union of open intervals. Thus, we need to show that any open interval can be represented by elements of $\mathcal{S}$. Let $-\infty < a <  b < \infty$.
\[(a,b) = (-\infty, a]^c \cup (\cup_i(-\infty, b - 1/n])\]
If $a = -\infty$, then $(a,b)$ can be represented by the second term. If $b =  \infty$, then $(a,b)$ can be represented by the first term. Thus, $\sigma(\mathcal{S}) = \sigma(\mathcal{R}) = \mathcal{R}$.

ii) $d \ge 2$. $\mathcal{S}_d$ is a finite cartesian product of $\mathcal{S}$. Similarly, $\sigma(\mathcal{S}_d) = \sigma(\mathcal{R}_d) = \mathcal{R}_d$.
\end{proof}
\begin{ex}
A $\sigma$-field F is said to be countably generated if there is a countable collection $C \subset F$ so that $\sigma(C) = F$. Show that $\mathcal{R}^d$ is countably generated.
\end{ex}
\begin{proof}[sol]
Let $C$ be the collection that contains all sets of the form
\[[q_1,\infty)\times \dotsb\times[q_d,\infty), (q_1,\dotsc,q_d) \in \mathbb{Q}^d\]
Then, $C$ is countable, since it is finite union of countable sets. And $\sigma(C) = \mathcal{R}^d$ as presented by previous exercise.
\end{proof}
\begin{ex}
(i) Show that if $F_1 \subset F_2 \subset \dotsc $ are $\sigma$-algebras, then $\cup_i{F}_i$ is an algebra. (ii) Give an example to show that $\cup_iF_i$ need not be a $\sigma$-algebra.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] By definition, $\cup_i F_i$ is not empty.
    Choose $x \in \cup_i F_i$. Then, there exists $F_x$ such that $x \in F_x$. Therefore, $x^c \in F_x \subset \cup_i F_i$. Thus, $\cup_i F_i$ is closed under complementations.
    
    Choose, $x \in F_i, y \in F_j$ and suppose $i\le j$. Then $x \in F_j$. Thus, $x \cup y \in F_j \Rightarrow x \cup y \in \cup_i F_i$.  Thus, $\cup_i F_i$ is closed under union.
    \item[(ii)] Let $F_i = \sigma(\{\{1\},\{2\},\dotsb,\{n\}\})$. Let $A = \{\{n\} : n = 3k| k = 1,2,3,\dotsb\}$. Then for all $i$, $A \notin F_i$. Thus, $A \not\in \cup_i F_i$. However, $A$ can be represented by countable union. Therefore, $\cup_iF_i$ is not a $\sigma-$algebra.
\end{enumerate}
\end{proof}
\begin{ex}
A set $A \subset \{1, 2, \dotsc \}$ is said to have asymptotic density $\theta$ if
\[\lim_{n\to\infty} |A \cap \{1, 2,\dotsb , n\}|/n = \theta\]
Let $\mathcal{A}$ be the collection of sets for which the asymptotic density exists.
Is $\mathcal{A}$ a $\sigma$-algebra? an algebra?
\end{ex}
\begin{proof}[sol]
Let $A$ be the set of even numbers. Next, we construct a set $B$ in the following way: we begin with $\{2, 3\}$ and starting with $k = 2$, take all even numbers $2^k < n \le (3/2) \times 2^k$
, and all odd numbers$(3/2) \times 2^k < n \le 2^{k+1}$. Then, the asymptotic density of $B$ is 0.5. However, the asymptotic density $A\cap B$ does not exists.

When $n = (3/2)\times 2^k$, then the density is 1/3. When $n = 2^{k+1}$, then the density is 1/4.

Thus, $\mathcal{A}$ is not closed under intersection. $\mathcal{A}$ is neither $\sigma-$algebra nor algebra
\end{proof}
\section{Distributions}
\begin{ex}
Suppose $X$ and $Y$ are random variables on $(\Omega,\mathcal{F}, P)$ and let $A \in \mathcal{F}$. Show that if we let $Z(\omega) = X(\omega)$ for $\omega \in A$ and $Z(\omega) = Y (\omega)$ for $\omega \in A^c$, then $Z$ is a random variable.
\end{ex}
\begin{proof}[sol]
Let the Borel set $B$ which satisfies that
\[\text{if } \omega \in A, \text{ then } Z(\omega) \in B\]
For arbitrary Borel set $S$, $S = (S\cap B)\cup (S\cap B^c)$. Since $S$ is Borel set, $\{\omega : X(\omega) \in S\}, \{\omega : Y(\omega) \in S\} \in \mathcal{F}$.
\begin{align*}
    \{\omega : Z(\omega) \in S\} &= \{\omega : Z(\omega) \in (S\cap B)\} \cup \{\omega : Z(\omega) \in (S\cap B^c)\}\\
    &= \{\omega : X(\omega) \in (S\cap B)\} \cup \{\omega : Y(\omega) \in (S\cap B^c)\}
\end{align*}
Since $\mathcal{F}$ is closed on set operation, $\{\omega : Z(\omega) \in S\} \in \mathcal{F}$. 
\end{proof}
\begin{ex}
Let $\chi$ have the standard normal distribution. Use Theorem 1.2.6 to get upper and lower bounds on P($\chi \ge $ 4).
\end{ex}
\begin{proof}[sol]
\begin{align*}
    P(\chi \ge 4) = (2\pi)^{-1}\int_4^\infty \exp(-y^2/2)dy &\le (8\pi)^{-1}\exp(-8)\\
    (2\pi)^{-1}\int_4^\infty \exp(-y^2/2)dy &\ge (15/128\pi)\exp(-8)
\end{align*}
\end{proof}
\begin{ex}
Show that a distribution function has at most countably many discontinuities.
\end{ex}
\begin{proof}[sol]
Let $D$ be the set of discontinuity points. Choose $x, y \in D$. Then we can choose rational number $q_x \in (F(x-), F(x+))$. Since $F$ is increasing, if $x\ne y$, then $q_x \ne q_y$. Thus $x \to q_x$ is one-to-one function. Since $\mathbb{Q}$ is countable, $D$ is at most countable.
\end{proof}
\begin{ex}
Show that if $F(x) = P(X \le x)$ is continuous then $Y = F(X)$ has a uniform distribution on (0,1), that is, if $y \in [0, 1]$, $P(Y \le y) = y$.
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \{\omega | Y(\omega) \le y\} &= \{\omega | F(X(\omega)) \le y\}\\
    &= \{\omega | X(\omega) \le k\} \quad k = \inf\{x | F(x) \ge y \}\\
    P(\{\omega | Y(\omega) \le y\}) &= P(\{\omega | X(\omega) \le k\})\\
    &= P(X \le k) = y
\end{align*}
\end{proof}
\begin{ex}
Suppose $X$ has continuous density $f$, $P(\alpha \le X \le \beta) = 1$ and $g$ is a function that is strictly increasing and differentiable on $(\alpha, \beta)$. Then $g(X)$ has density $f(g^{-1}(y))/g'(g^{-1}(y))$ for $y \in (g(\alpha), g(\beta))$ and 0 otherwise. When $g(x) = ax + b$ with $a > 0, g^{-1}(y) = (y - b)/a$ so the answer is $(1/a)f((y - b)/a)$.
\end{ex}
\begin{proof}[sol]
Since $g$ is strictly increasing $g^{-1}$ exists.
\begin{align*}
    P(g(X) \le y) &= P(X \le g^{-1}(y)) = \int_\alpha^{g^{-1}(y)} f(x)dx\\
    \frac{d}{dy}P(g(X) \le y) &= f(g^{-1}(y))\frac{d}{dy}g^{-1}(y) = f(g^{-1}(y))/g'(g^{-1}(y))
\end{align*}
\end{proof}
\begin{ex}
Suppose $X$ has a normal distribution. Use the previous exercise to compute the density of $\exp(X)$.
\end{ex}
\begin{proof}[sol]
Let $g(x) = \exp(x)$. Then $g^{-1}(x) = \log(x)$
\begin{align*}
    f(g^{-1}(x))/g'(g^{-1}(x)) &= f(\log(x))/\exp(\log(x))\\
    &= f(\log(x))/x = \frac{1}{x\sqrt{2\pi}}\exp(-\log(x)^2/2), \ x > 0
\end{align*}
\end{proof}
\section{Random Variables}
\begin{ex}
Show that if $\mathcal{A}$ generates $\mathcal{S}$, then $X^{-1}(\mathcal{A}) \equiv \{\{X \in A\} : A \in \mathcal{A}\}$ generates $\sigma(X) = \{\{X \in B\} : B \in \mathcal{S}\}$.
\end{ex}
\begin{proof}[sol]
By assumption, $\mathcal{A} \subset \mathcal{S}$. Thus,  $X^{-1}(\mathcal{A})\subset X^{-1}(\mathcal{S})$. Therefore, $\sigma(X^{-1}(\mathcal{A}))\subset\sigma( X^{-1}(\mathcal{S})) = \sigma(X)$.

Choose a set $B \in \mathcal{S}$. Since $\mathcal{A}$ generates $\mathcal{S}$, $B$ can be represented by sets of $\mathcal{A}$. Thus, $\{X \in B\} \in X^{-1}(\mathcal{A})$. Therefore, $\sigma(X) \in X^{-1}(\mathcal{A})$.
\end{proof}
\begin{ex}
Prove Theorem 1.3.6 when $n = 2$ by checking $\{X_1+X_2 < x\} \in \mathcal{F}$.
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \{X_1+X_2 < x\} = \bigcup_{q \in \mathbb{Q}} \{X_1 < q\}\times \{X_2 < x-q\}
\end{align*}
Both $\{X_1 < q\}$ and $\{X_2 < x-q\}$ are open sets. Thus, $\{X_1+X_2 < x\} \in \mathcal{R}^2$ since it is represented by countable union.
\end{proof}
\begin{ex}
Show that if $f$ is continuous and $X_n \to X$ almost surely then $f(X_n) \to f(X)$ almost surely.
\end{ex}
\begin{proof}[sol]
Let $\Omega_0 = \{\omega : \lim X_n(\omega) = X(\omega)\}$ and $\Omega_f = \{\omega : \lim f(X_n(\omega)) = f(X(\omega))\}$.
\begin{align*}
    \omega \in \Omega_0 &\Rightarrow \lim X_n(\omega) = X(\omega)\\
    &\Rightarrow \lim f(X_n(\omega)) = f(X(\omega))  \ \ \because f\text{ is continuous}\\
    &\Rightarrow \omega \in \Omega_f\\
    &\Rightarrow 1 = P(\Omega_0) \le P(\Omega_f)\le 1
\end{align*}
\end{proof}
\begin{ex}
(i) Show that a continuous function from $\mathcal{R}^d \to \mathcal{R}$ is a measurable map from ($\mathbb{R}^d,\mathcal{R}^d)$ to $(\mathbb{R},\mathcal{R})$. (ii) Show that $\mathcal{R}^d$ is the smallest $\sigma$-field that makes all the continuous functions measurable.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] $\mathcal{R}$ is $\sigma$-field which is generated by all open sets. If $f$ is continuous mapping and image is open, then inverse-image is also open. Thus, $f$ is a measurable map.
    \item[(ii)] Let $\mathcal{S}$ be the smallest $\sigma-$field that makes all the continuous functions measurable. Since $\mathcal{R}$ is the smallest $\sigma-$field which is generated by all open sets,  $\mathcal{S}$ is generated by all open sets in $\mathbb{R}^d$.
\end{enumerate}
\end{proof}
\begin{ex}
A function $f$ is said to be lower semicontinuous or l.s.c. if
\[\liminf_{y\to x} f(y) \ge f(x)\]
and upper semicontinuous (u.s.c.) if $-f$ is l.s.c.. Show that f is l.s.c. if
and only if $\{x : f(x) \le a\}$ is closed for each $a \in \mathbb{R}$ and conclude that
semicontinuous functions are measurable.
\end{ex}
\begin{proof}[sol]~
Define $E_a := \{x : f(x) \le a\}$
\begin{enumerate}
    \item[if] Choose $x$. Let $a = \liminf_{y\to x}{f(y)}$. Then, by definition, $a \le f(x)$.
    There is the sequence $\{x_n\}$ such that $x_n \to x, f(x_n) \to a$ as $n \to \infty$
    For any $b > a$, there exists $N_b$ such that for all $n > N_b$, $x_n \in E_b$. Since $E_b$ is closed, the limit point $x \in E_b$ for all $b > a$. Thus, $x \in E_a$. Since $f(x) \ge a$ and $f(x) \le a$, $f(x) = a = \liminf_{y\to x} f(y)$. 
    \item[only if] Construct a sequence $\{x_n\}$ in $E_a$ which converges to $x$. Since $f$ is l.s.c
    \[f(x) \le \liminf_{y\to x} f(y) \le a\]
    Thus, $x \in E_a$. Since an arbitrary limit point is in $E_a$, $E_a$ is closed.
\end{enumerate}
\end{proof}
\section{Integration}
\begin{ex}
Show that if $f \ge 0$ and $\int f d\mu = 0$ then $f = 0$ a.e.
\end{ex}
\begin{proof}[sol]
Let $N$ be the set that satisfies $x \in N \Rightarrow f(x) = 0$.
\begin{align*}
    \int f d\mu &=  \int_N f d\mu  + \int_{N^c} f d\mu\\
    &= \int_{N^c} f d\mu = 0\\
    &\Rightarrow \mu(N^c) = 0
\end{align*}
\end{proof}
\begin{ex}
Let $f \ge 0$ and $E_{n,m} = \{x : m/2^n \le f(x) < (m+1)/2^n\}$. As $n \uparrow \infty$,
\[\sum_{m=1}^\infty \frac{m}{2^n} \mu(E_{n,m}) \uparrow \int f d\mu\]
\end{ex}
\begin{proof}[sol]
Let $f_n = \sum_{m=1}^\infty 1_{E_{n,m}}$. Then, for any n, $f_n < f$. Thus, $\int f d\mu$ is an upper bound of $\int f_n d\mu$. Since $\int f_n d\mu$ is increasing and bounded, it converges. By definition,
\begin{align*}
    \int f -f_n d\mu \le \int 1/2^n d\mu \to 0 \text{ as } n \to \infty
\end{align*}
Thus, $\int f_n d\mu$ converges to $\int f d\mu$
\end{proof}
\begin{ex}
Let $g$ be an integrable function on $\mathbb{R}$ and $\epsilon > 0$. (i) Use the definition of the integral to conclude there is a simple function $\varphi = \sum_k b_k1_{A_k}$ with $\int |g - \varphi| dx < \epsilon$. (ii) Use Exercise A.2.1 to approximate the $A_k$ by finite unions of intervals to get a step function
\[q = \sum_{j=1}^k c_j 1_{(a_{j-1}, a_j)}\]
with $a_0 < a_1 < \dotsb < a_k$, so that $\int |\varphi-q| < \epsilon$. (iii) Round the corners of $q$ to get a continuous function $r$ so that $\int |q - r| dx < epsilon$.
(iv) To make a continuous function replace each $c_j1_{(a_{j-1},a_j)}$ by a function
that is 0 $(a_{j-1}, a_j)^c$, $c_j$ on $[a_{j-1} + \delta - j, a_j - \delta_j ]$, and linear otherwise. If the $\delta_j$ are small enough and we let $r(x) = \sum^k_{j=1} r_j(x)$ then
\[\int |q(x) - r(x)|d\mu = \sum_{j=1}^k \delta_j c_j <\epsilon\]
\end{ex}
\begin{ex}
Prove the Rimann-Lebesgue lemma. If $g$ is integrable then
\[\lim_{n\to\infty} \int g(x) \cos nx dx  = 0\]
\end{ex}
\begin{proof}[sol]
Let $\epsilon$ be an arbitrary positive number. We can find a step function $\varphi$ such that $\int |g - \varphi|dx < \epsilon / 2$.
\begin{align*}
    \int \varphi(x)\cos(nx)dx &= \sum_{i=1}^m c_i\int_{a_i}^{b_i}\cos(nx)dx\\
    &= \sum_{i=1}^m c_i/n\int_{na_i}^{nb_i}\cos(y)dy\\
    &= \sum_{i=1}^m c_i/n(\sin(nb_i) -\sin(na_i)) \to 0
\end{align*}
Thus, we can choose $N$ which satisfies that for all $m \ge N$, $\int|\varphi(x)\cos(nx)|dx < \epsilon/2$. Then,
\begin{align*}
    |\int g(x)\cos(nx)dx| &\le \int|g(x)\cos(nx)|dx\\
    &\le \int|g(x)-\varphi(x)||\cos(nx)|dx + \int|\varphi(x)\cos(nx)|dx\\
    &\le \epsilon/2 + \epsilon/2 = \epsilon
\end{align*}
Since $\epsilon$ is arbitrary, it converges to 0.
\end{proof}
\section{Properties of the integral}
\begin{ex}
Let $\|f\|_\infty = \inf\{M : \mu(\{x : |f(x)| > M\}) = 0\}$. Prove that
\[\int |fg| d\mu \le \|f\|_1\|g\|_\infty\]
\end{ex}
\begin{proof}[sol]
By definition, $g \le \|g\|_\infty$ a.e.. Thus,
\[\int |fg| d\mu \le \int |f|\|g\|_\infty d\mu = \|g\|_\infty \int|f|d\mu = \|f\|_1\|g\|_\infty\]
\end{proof}
\begin{ex}
Show that if $\mu$ is a probability measure then
\[\|f\|_\infty = \lim_{p\to\infty} \|f\|_p\]
\end{ex}
\begin{proof}[sol]
Since $\mu$ is a probability measure, $\mu(\Omega) = 1$.
Suppose two positive numbers, $n < m$. Let $\varphi(x) := x^{m/n}$ By Jensen's inequality,
\[\|f\|_n^m = \varphi(\int |f|^nd\mu) \le \int \varphi(|f|^n)d\mu  = \|f\|_m^m\]
Thus, $\{\|f\|_n\}$ is an increasing sequence.
\[\|f\|_n^n = \int |f|^nd\mu \le \int (\|f\|_\infty^n)d\mu  = \|f\|_\infty^n\]
Thus, $\|f\|_\infty$ is an upper bound. Therefore, the sequence converges and $\lim_{p\to\infty} \|f\|_p \le\|f\|_\infty$.

Let $\epsilon$ be the arbitrary number between 0 and $\|f\|_\infty$. Then, there exist a set $M$ such that $|f| > \|f\|_\infty - \epsilon$ for all $x \in M$.
\begin{align*}
    \int |f|^p d\mu &\ge \int_M |f|^p d\mu \ge (\|f\|_\infty - \epsilon)^p\mu(M)\\
    \Rightarrow \|f\|_p &\ge (\|f\|_\infty - \epsilon)\mu(M)^{1/p}\\
    \Rightarrow \lim_{p\to\infty} \|f\|_p &\ge \|f\|_\infty - \epsilon\\
    \Rightarrow \lim_{p\to\infty} \|f\|_p &\ge \|f\|
\end{align*}
Thus, the sequences converges to $\|f\|_\infty$.
\end{proof}
\begin{ex}[Minkowski's inequality]
(i) Suppose $p \in (1,\infty)$. The inequality $|f+g|^p \le 2^p(|f|^p + |g|^p)$ shows that if $\|f\|^p$ and $\|g\|^p$ are $<\infty$ then $\|f+g\|_p < \infty$. Apply Holder's inequality to $|f||f+g|^{p-1}$ and $|g||f+g|^{p-1}$ to show $\|f+g\|_p \le \|f\|_p + \|g\|_p$ (ii) Show that the last result remains true when $p = 1$ or $p = \infty$
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] By Holder's inequality,
    \begin{align*}
    \int |f||f+g|^{p-1}d\mu &\le \|f\|_p\|(f+g)^{p-1}\|_{p/(p-1)}\\
    &= (\int |f|^pd\mu)^{1/p}(\int |f+g|^pd\mu)^{(p-1)/p}\\
    \text{Similarly, }\int |g||f+g|^{p-1}d\mu &\le (\int |g|^pd\mu)^{1/p}(\int |f+g|^pd\mu)^{(p-1)/p}\\
    \int |f+g|^pd\mu &= \int (|f|+|g|)|f+g|^{p-1}d\mu \le (\|f\|_p+\|g\|_p)\|f+g\|_p^{p-1}\\
    \Rightarrow \|f+g\|_p &\le(\|f\|_p+\|g\|_p)
    \end{align*}
    \item[(ii)] When $p = 1$, it is trivial.
    
    Let the set $E = \{M: \mu(\{x : |f(x)+g(x)| > M\}) = 0\}$. By triangle inequality,
    \[|f+g| \le |f| + |g| \le \|f\|_\infty +\|g\|_\infty \ a.e.\]
    Thus,
    \begin{align*}
        \mu(\{x : |f(x)+g(x)| > \|f\|_\infty + \|g\|_\infty\}) = 0
    \end{align*}
    Since $\|f\|_\infty +\|g\|_\infty\in E$, $\|f\|_\infty +\|g\|_\infty \ge \inf E =\|f+g\|_\infty$
\end{enumerate}
\end{proof}
\begin{ex}
If $f$ is integrable and $E_m$ are disjoint sets with union $E$ then
\[\sum_{m=0}^\infty \int_{E_m} f d\mu = \int_E fd\mu\]
So if $f \ge 0$, then $\nu(E) = \int_E fd\mu$ defines a measure.
\end{ex}
\begin{proof}[sol]~
If $E = \emptyset$, $\mu(E) = 0$. Since $f \ge 0$, for any set $E$, $\nu(E) \ge \nu(\emptyset) = 0$.

If $E_i$ are disjoint sets, then
\[\mu(\cup_i E_i) = \int_{\cup_i E_i} fd\mu = \sum_i\int_{E_i}fd\mu = \sum_i \nu(E_i)\]

Thus, $\nu$ is a measure.
\end{proof}
\begin{ex}
If $g_n \uparrow g$ and $\int g_1^- d\mu < \infty$ then $\int g_n d\mu \uparrow \int gd\mu$.
\end{ex}
\begin{proof}[sol]
Since $g_n = g_n^+ - g_n^-$ and $g_n \uparrow g$, $g_n^+ \uparrow g^+$ and $g_n^- \downarrow g^-$. By monotone convergence theorem, $\int g_n^+d\mu \uparrow \int g^+d\mu$. Since $g_1^-$ is integrable, by dominated convergence theorem, $\int g_n^-d\mu\to \int g^-d\mu$. Thus, $-\int g_n^-d\mu\uparrow -\int g^-d\mu$.
\[\therefore \int g_nd\mu = \int g_n^+ - g_n^- d\mu \uparrow \int gd\mu\]
\end{proof}
\begin{ex}
If $g_m \ge 0$ then $\int \sum_{m=0}^\infty g_m d\mu = \sum_{m=0}^\infty \int g_m d\mu$.
\end{ex}
\begin{proof}[sol]
Define $f_n := \sum_{m=0}^n g_m$ and $f := \sum_{m=0}^\infty g_m$. Then $f_n \uparrow f$. By result of using previous example, $\int f_nd\mu \uparrow \int fd\mu$. Thus,
\begin{align*}
    \int \sum_{m=0}^\infty g_m d\mu &=\int \lim_{n\to\infty}f_nd\mu\\
    &= \int fd\mu\\
    &= \int \sum_{m=0}^\infty g_md\mu
\end{align*}
\end{proof}
\begin{ex}
Let $f \ge 0$. (i) Show that $\int f \land n \ d\mu \uparrow \int fd\mu$ as $n\to\infty$. (ii) Use (i) to conclude that if $g$ is integrable and $\epsilon > 0$ then we can pick $\delta > 0$ so that $\mu(A) < \delta$ implies $\int_A |g|d\mu < \epsilon$.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item[(i)] 
Define $E_n := \{x : f(x) > n\}$. Then, for $n \le m$
\begin{align*}
   &f \land m -  f \land n = (f-n) 1_{E_n \cap E_m^c} + (m-n)1_{E_m} \ge 0\\
   &\lim_{n\to\infty} f \land n = f
\end{align*}
By definition $f_n \ge 0$. Thus, by monotone convergence theorem, $\int f \land n \ d\mu \uparrow \int fd\mu$.
\item[(ii)] By results of (i), we can find the positive $N$ for any positive $\epsilon/ 2$, $\mu(\{x : |g(x)| > N\}) < \epsilon$. 
\begin{align*}
    \int_A |g|d\mu &\le \int_{\{x : |g(x)| > N\}} |g|d\mu +  \int_{\{x \in A : |g(x)| \le N\}} |g|d\mu\\
    &\le \epsilon /2 + N \mu(A)
\end{align*}
Thus, if we set $\delta := \epsilon /(2N)$, then $\mu(A) < \delta$ implies $\int_A |g|d\mu < \epsilon$.
\end{enumerate}
\end{proof}
\begin{ex}
Show that if $f$ is integrable on $[a,b]$, $g(x) = \int_{[a,x]}f(y)dy$ is continuous on $(a,b)$.
\end{ex}
\begin{proof}[sol]
Since $f$ is integrable, there exists a positive number $M$ such that $|f| < M$. For $s,t \in (a,b), s\le t$
\begin{align*}
-M(t-s) < g(t) - g(s) &= \int^t_s f(y)dy < M(t-s)\\
\Rightarrow |g(t) - g(s)| &< M(t-s)
\end{align*}
Thus, $g$ is Lipschitz continuous. Thus, $g$ is continuous.
\end{proof}
\begin{ex}
Show that if $f$ has $\|f\|_p = (\int |f|^pd\mu)^{1/p} < \infty$, then there are simple functions $\varphi_n$ so that $\|\varphi_n - f\|_p \to 0$.
\end{ex}
\begin{proof}[sol]
For $f$, we can construct sequence of step functions $\{\varphi_n\}$ such that $\varphi_n \uparrow |f|$. By triangle inequailty, $|f - \varphi_n| \le |f| + |\varphi_n|$. Thus, for all $n$
\begin{align*}
    |f - \varphi_n|^p \le (|f| + |\varphi_n|)^p \le (2|f|)^p
\end{align*}
By dominated convergence theorem,
\begin{align*}
    \lim_{n\to\infty}\int |f - \varphi_n|^p &= \int \lim_{n\to\infty}|f - \varphi_n|^p = 0\\
    \Rightarrow \|f - \varphi_n\|_p &= 0
\end{align*}
\end{proof}
\begin{ex}
Show that if $\sum_n \int |f_n|d\mu < \infty$, then $\sum_n \int f_n d\mu = \int \sum_n f_n d\mu$.
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \sum_n \int |f_n|d\mu &= \sum_n \int f_n^+d\mu + \sum_n \int f_n^-d\mu\\
    &\Rightarrow \sum_n \int f_n^+d\mu<\infty, \sum_n \int f_n^-d\mu < \infty
\end{align*}
By the result of (1.5.6),
\begin{align*}
    \sum_n \int f_n^+d\mu &= \int \sum_n f_n^+d\mu\\
    \sum_n \int f_n^-d\mu &= \int \sum_n f_n^-d\mu\\
    \sum_n \int f_nd\mu &= \sum_n \int f_n^+ - f_n^-d\mu\\
    &= \int \sum_n f_n^+d\mu - \int \sum_n f_n^-d\mu\\
    &= \int \sum_n f_nd\mu
\end{align*}
\end{proof}

\section{Expected Value}
\begin{ex}
Suppose $\varphi$ is strictly convex, i.e., $>$ holds for $\lambda \in (0,1)$. Show that, under the assumptions of Theorem 1.6.2, $\varphi(EX) = E(\varphi X)$ implies $X = EX$ a.s.
\end{ex}
\begin{proof}[sol]
Let $t = EX$. Since $\varphi$ is strictly convex, we can find $a$ which satisfies, for all $x$, $\varphi(x) \ge \varphi(t) + a(x -t)$. Thus $E\varphi(X) \ge \varphi(t) + E[a(X - t)] = \varphi(EX)$. If we add assumption $\varphi(EX) = E(\varphi X)$, then we can conclude $a(X - t) = 0$ a.e. Thus, $X = t = EX$ a.e. 
\end{proof}
\begin{ex}
Suppose $\varphi : \mathbb{R}^n \to \mathbb{R}$ is convex. Imitate the proof of Theorem 1.5.1 to show
\[E \varphi(X_1,...,X_n) \ge \varphi(EX_1,...,EX_n)\]
provided $E|E \varphi(X_1,...,X_n)| < \infty$ and $E|X_i| < \infty$ for all $i$.
\end{ex}
\begin{ex}[Chebyshev's inequality is and is not sharp]
(i) Show that Theorem 1.6.4 is sharp by showing that if $0 < b \le a$ are fixed there is an $X$ with $EX^2 = b^2$ for which $P(|X| \ge a) = b^2/a^2$. (ii) Show that Theorem 1.6.4 is not sharp by showing that if $X$ has $0 < EX^2 <\infty$ then
\[\lim_{a\to\infty} a^2 P(|X| \ge a)/EX^2 = 0\]
\end{ex}
\begin{ex}[One-sided Chebyshev bound]
(i) Let $a > b > 0, 0 < p < 1$, and let $X$ have $P(X = a) = p$ and $P(X = -b) = 1-p$. Apply Theorem 1.6.4 to $\varphi(x) = (x+b)^2$ and conclude that if $Y$ is any random variable with $EY = EX$ and $Var (Y) = Var (X)$, then $P(Y \ge a) \le p$ and equality holds when $Y = X$.
(ii) Suppose $EY = 0$, $Var(Y) = \sigma^2$, and $a > 0$. Show that $P(Y \ge a) \le \sigma^2 /(a^2 + \sigma^2)$, and there is a $Y$ for which equality holds.
\end{ex}
\begin{ex}[Two nonexistent lower bounds]
Show that : (i) if $\epsilon > 0$, $\inf \{P(|X| > \epsilon) : EX = 0, Var(X) = 1\} = 0$. (ii) if $y \ge 1, \sigma^2 \in (0,\infty)$, $\inf \{P(|X| > y) : EX = 1, Var(X) = \sigma^2\} = 0$
\end{ex}
\begin{proof}[sol]
\begin{enumerate}
    \item[(i)]
    \begin{align*}
        P(|X| > \epsilon) \le E(|X|)/\epsilon
    \end{align*}
\end{enumerate}
\end{proof}
\begin{ex}[A useful lower bound]
Let $Y \ge 0$ with $EY^2 <\infty$. Apply the Cauchy-Schwarz inequality to $Y 1_{Y >0}$ and conclude
\[P(Y > 0) \ge (EY)^2 / EY^2\]
\end{ex}
\begin{proof}[sol]
\begin{align*}
    E[Y1_{Y > 0}] &\le \sqrt{EY^2E[1_{Y>0}]}\\
    EY - 0\cdot P(Y = 0) &\le \sqrt{EY^2P(Y >0)}\\
    (EY)^2 &\le EY^2P(Y > 0)\\
    P(Y > 0) &\ge (EY)^2 / EY^2
\end{align*}
\end{proof}
\begin{ex}
Let $\Omega = (0,1)$ equipped with the Borel sets and Lebesgue measure. Let $\alpha \in (1,2)$ and $X_n = n^\alpha 1_{(1/(n+1), 1/n)} \to 0$ a.s. Show that Theorem 1.6.8 can be applied with $h(x) = x$ and $g(x) = |x|^{2/\alpha}$, but the $X_n$ are not dominated by an integrable function.
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item $g \ge 0$, $\because 2/\alpha > 1$, $g(x) \to \infty$ as $|x| \to \infty$
    \item $|h(x)|/g(x) = |x|^{(\alpha -2)/2}$, $\because (\alpha - 2) < 1$, $|h(x)|/g(x) \to 0$ as $|x| \to \infty$.
    \item $Eg_n(X) = n^2\int 1_{(1/n+1, 1/n)}d\mu  = n/(n+1) < 1$.
\end{enumerate}
Thus, $Eh(X_n) \to Eh(X)$. However, there is no integrable function which dominates $X_n$ because there is no upper bound for $\sup_n |X_n|$.
\end{proof}
\begin{ex}
Suppose that the probability measure $\mu$ has $\mu(A) = \int_A f(x)dx$ for all $A \in \mathcal{R}$. Use the proof technique of Theorem 1.6.9 to show that for any $g$ with $g \ge 0$ or $\int |g(x)|\mu(dx) < \infty$ we have
\[\int g(x)\mu(dx) = \int g(x)f(x)dx\]
\end{ex}
\begin{proof}[sol]~
\begin{enumerate}
    \item Indicator function
    
    Let $g = c1_{A}$. Then,
    \[\int g(x)\mu(dx) = \int_A c\mu(dx) = c\mu(A) = \int_A cf(x)dx = \int g(x)f(x)dx\]
    \item Simple function
    
    Let $g = \sum_{i=1}^n c_i1_{A_i}$. Then, by case 1,
    \[\int g(x)\mu(dx) = \sum_{i=1}^n  c_i\mu(A_i) = \int g(x)f(x)dx\]
    \item Nonnegative function
    
    There is a sequence of simple functions $\{g_n\}$ which satisfies that $g_n \uparrow g$. By monotone convergence theorem,
    \[\int g(x)\mu(dx) = \lim_{n\to\infty} \int g_n(x)\mu(dx) = \lim_{n\to\infty} \int g_n(x)f(x)dx = \int g(x)f(x)dx\]
    
    The proof ends if $g \ge 0$.
    \item Integrable function($g$ is integrable)
    
    Let $g = g^+ - g^-$. Then
    \begin{align*}
        \int g^+(x)\mu(dx) &= \int g^+(x)f(x)dx, \int g^-(x)\mu(dx) = \int g^-(x)f(x)dx\\
        \Rightarrow \int g(x)\mu(dx) &=  \int g^+(x)\mu(dx) - \int g^-(x)\mu(dx)\\
        &=  \int g^+(x)f(x)dx - \int g^-(x)f(x)dx = \int g(x)f(x)dx
    \end{align*}
    
    
\end{enumerate}
\end{proof}
\begin{ex}[Inclusion-exclusion formula]
Let $A_1,A_2,...,A_n$ be events and $A = \cup_{i=1}^n A_i$. Prove that $1_A = 1 - \prod_{i=1}^n (1 - 1_{A_i})$. Expand out the right hand side, then take expected value to conclude
\begin{align*}
    P(\cup_{i=1}^n A_i) = &\sum_{i=1}^n P(A_i) - \sum_{i < j}P(A_i \cap A_j)\\
    &+ \sum_{i < j < k} P(A_i \cap A_j \cap A_k) - ... + (-1)^{n-1}P(\cap_{i=1}^n A_i)
\end{align*}
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \omega \in A &\Leftrightarrow \omega \in \cup_{i=1}^n A_i \Leftrightarrow \omega \not\in \cap_{i=1}^n A_i^C\\
    \Rightarrow 1_A(\omega) &= 1 - \prod_{i=1}^n(1 - 1_{A_i}(\omega)) 
\end{align*}
\end{proof}
\begin{ex}
\end{ex}
\begin{ex}
If $E|X|^k < \infty$ then for $0 < j < k$, $E|X|^j < \infty$, and furthermore
\[E|X|^j \le (E|X|^k)^{j/k}\]
\end{ex}
\begin{proof}[sol]
Define $\varphi(x) = x ^{k/j}$. Then $\varphi$ is a convex function. By Jensen's inequality
\begin{align*}
    (E|X|^j)^{k/j} = \varphi(E|X|^j) &\le E\varphi(|X|^j) = E|X|^k\\
    \Rightarrow E|X|^j &\le (E|X|^k)^{j/k}
\end{align*}
\end{proof}
\begin{ex}
Apply Jensen's inequality with $\varphi(x) = e^x$ and $P(X = \log y_m) = p(m)$ to conclude that if $\sum_{m=1}^n p(m) = 1$ and $p(m), y_m > 0$ then
\[\sum_{m=1}^n p(m)y_m \ge \prod_{m=1}^n y_m^{p(m)}\]
When $p(m) = 1/n$, this says the arithmetic mean exceeds the geometric mean.
\end{ex}
\begin{proof}[sol]
\begin{align*}
E\varphi(X) &= \sum_{n=1}^m p(m) \exp(\log y_m) = \sum_{n=1}^m p(m)y_m\\
\varphi(EX) &= \exp(\sum_{n=1}^m p(m)\log y_m) = \prod_{n=1}^m y_m^{p(m)}\\
\end{align*}
By Jensen's inequality,
\[\sum_{m=1}^n p(m)y_m  = E\varphi(X) \ge \varphi(EX) = \prod_{m=1}^n y_m^{p(m)}\]
If $p(m) = 1/n$, the left hand becomes arithmetic mean and the right hand becomes geometric mean.
\end{proof}
\begin{ex}
If $EX_1^- <\infty$ and $X_n \uparrow X$ then $EX_n \uparrow EX$.
\end{ex}
\begin{proof}[sol]
Same with Exercise 1.5.5. We use the probability measure in this exercise.
\end{proof}
\begin{ex}
Let $X \ge 0 $ but do NOT assume $E(1/X) < \infty$. Show
\[\lim_{y\to\infty} yE(1/X; X>y) = 0,\quad \lim_{y\downarrow 0} yE(1/X; X>y) = 0\]
\end{ex}
\begin{proof}[sol]
\begin{align*}
    0 \le yE(1/X;X>y) &= \int_{\{\omega : X(\omega) > y\}} y/X d\mu\\
    &\le \int_{\{\omega : X(\omega) > y\}} 1d\mu\\
    &= \mu(\{\omega : X(\omega) > y\})\\
    \Rightarrow\lim_{y\to\infty} yE(1/X; X>y) &\le \lim_{y\to\infty}\mu(\{\omega : X(\omega) > y\}) = 0
\end{align*}
\end{proof}
\begin{ex}
If $X_n \ge 0$ then $E(\sum_{n=0}^\infty X_n) = \sum_{n=0}^\infty EX_n$.
\end{ex}
\begin{proof}[sol]
Same with Exercise 1.5.10.  We use the probability measure in this exercise.
\end{proof}
\begin{ex}
If $X$ is integrable and $A_n$ are disjoint sets with union $A$ then
\[\sum_{n=0}^\infty E(X; A_n) = E(X; A)\]
i.e., the sum converges absolutely and has the value on the right.
\end{ex}
\begin{proof}[sol]
Define $Y_n := \sum_{i=0}^n X1_{A_i}$ and $Y:= X1_A$. Then, $Y_n \to Y$ and $|Y| \le |X|$. Since $X$ is integrable, $Y$ is also integrable. For any $\omega \in \cup_{i=0}^n A_i$, $Y_n(\omega) = Y(\omega)$. Thus, $|Y_n| \le |Y|$. By dominated convergence theorem,
\[\lim_{n\to\infty} EY_n  \to EY \]
\begin{align*}
    \lim_{n\to\infty} EY_n &= \lim_{n\to\infty} E(X;A_n)\\
    EY &= E(X;A)
\end{align*}
\end{proof}
\section{Product Measures, Fubini's Theorem}
\begin{ex}
If $\int_X\int_Y |f(x,y)|\mu_2(dy)\mu_1(dx) < \infty$ then
\[\int_X\int_Y |f(x,y)|\mu_2(dy)\mu_1(dx) = \int_{X\times Y}fd(\mu_1 \times \mu_2) =\int_Y\int_X |f(x,y)|\mu_1(dx)\mu_2(dy)\]
{\bf{Corollary}}. Let $X = \{1,2,...\}$, $\mathcal{A} = $ all subsets of $X$, and $\mu_1 = $ counting measure. If $\sum_n\int |f_n|d\mu < \infty$ then $\sum_n \int f_nd\mu = \int\sum_n f_nd\mu$.
\end{ex}
\begin{ex}
Let $g \ge 0$ be a measurable function on $(X,\mathcal{A},\mu)$. Use Theorem 1.7.2 to conclude that
\[\int_X gd\mu = (\mu \times \lambda)(\{(x,y) : 0 \le y < g(x)\}) = \int_0^\infty \mu(\{x : g(x) > y\})dy\]
\end{ex}
\begin{align*}
    \int_X gd\mu &= \int_X\int_\mathbb{R} 1_{0 \le y < g(x)}dyd\mu\\
    &= (\mu \times \lambda) (\{(x,y) : 0 \le y < g(x)\}\\
    &= \int_\mathbb{R}\int_X 1_{g(x) > y \ge 0}d\mu dy\\
    &= \int_\mathbb{R} \mu(\{x : g(x) > y \ge 0\})dy\\
    &= \int_0^\infty \mu(\{x : g(x) > y\})dy
\end{align*}
\begin{ex}
Let  $F, G$ be Stieltjes measure functions and let $\mu, \nu$ be the corresponding measures on $(\mathbb{R}, \mathcal{R})$. Show that
\end{ex}
\begin{ex}
Let $\mu$ be a finite measure on $\mathbb{R}$ and $F(X) = \mu((-\infty, x])$. Show that
\[\int (F(x+c) - F(x)) dx = c\mu(\mathbb{R})\]
\end{ex}
\begin{proof}[sol]
\begin{align*}
    \int (F(x+c) - F(x))dx &= \int \mu((x,x+c])dx\\
    &=\int\int 1_{(x,x+c]}(y) d\mu dx\\
    &=\int\int 1_{[y-c,y)}(x) dx d\mu\\
    &= \int cd\mu = c\mu(\mathbb{R})
\end{align*}
\end{proof}
\begin{ex}
Show that $e^{-xy}\sin x$ is integrable in the strip $0 < x < a$, $0 < y$. Perform the double integral in the two orders to get:
\[\int_0^a \frac{\sin x}{x}dx = \arctan(a) - (\cos a)\int_0^\infty \frac{e^{-ay}}{1+y^2}dy - (\sin a)\int_0^\infty \frac{ye^{-ay}}{1+y^2}dy\]
and replace $1+y^2$ by $1$ to conclude $|\int_0^a (\sin x)/x dx - \arctan(a)| \le 2/a$ for $a \ge 1$. 
\end{ex}

\end{document}
